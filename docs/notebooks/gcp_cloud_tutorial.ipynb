{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "gcp-title",
   "metadata": {},
   "source": [
    "# Google Cloud Platform (GCP) Tutorial\n",
    "\n",
    "This tutorial demonstrates how to use Clustrix with Google Cloud Platform (GCP) infrastructure for scalable distributed computing.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ContextLab/clustrix/blob/master/docs/notebooks/gcp_cloud_tutorial.ipynb)\n",
    "\n",
    "## Overview\n",
    "\n",
    "GCP provides several services that integrate well with Clustrix:\n",
    "\n",
    "- **Compute Engine**: Virtual machines for compute clusters\n",
    "- **Google Kubernetes Engine (GKE)**: Managed Kubernetes clusters\n",
    "- **Batch**: Managed job scheduling service\n",
    "- **Cloud Run**: Serverless container platform\n",
    "- **Vertex AI**: Machine learning platform\n",
    "- **Cloud Storage**: Object storage for data and results\n",
    "- **VPC**: Network isolation and security\n",
    "- **Preemptible VMs**: Cost-effective compute instances\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. Google Cloud account with billing enabled\n",
    "2. Google Cloud SDK (gcloud) installed and configured\n",
    "3. SSH key pair for VM access\n",
    "4. Basic understanding of GCP services"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "installation",
   "metadata": {},
   "source": [
    "## Installation and Setup\n",
    "\n",
    "Install Clustrix with GCP dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Clustrix with GCP support\n",
    "!pip install clustrix google-cloud-compute google-cloud-storage google-auth google-auth-oauthlib\n",
    "\n",
    "# Import required libraries\n",
    "import clustrix\n",
    "from clustrix import cluster, configure\n",
    "from google.cloud import compute_v1\n",
    "from google.cloud import storage\n",
    "from google.auth import default\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gcp-authentication",
   "metadata": {},
   "source": [
    "## GCP Authentication Setup\n",
    "\n",
    "Configure your GCP credentials. You can do this in several ways:\n",
    "\n",
    "### Option 1: gcloud CLI Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gcloud-auth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login with gcloud CLI (run this in terminal)\n",
    "# gcloud auth login\n",
    "# gcloud auth application-default login\n",
    "\n",
    "# Set your default project\n",
    "# gcloud config set project YOUR_PROJECT_ID\n",
    "\n",
    "# Verify authentication\n",
    "!gcloud auth list\n",
    "!gcloud config get-value project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gcp-service-account",
   "metadata": {},
   "source": [
    "### Option 2: Service Account Authentication"
   ]
  },
  {
   "cell_type": "code",
   "id": "service-account",
   "metadata": {},
   "outputs": [],
   "source": "# Set service account key path\n# os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/path/to/service-account-key.json'\n\n# Test GCP connection\ntry:\n    credentials, project_id = default()\n    print(f\"Successfully authenticated with project: {project_id}\")\n    \n    # Test compute API\n    compute_client = compute_v1.InstancesClient()\n    print(\"Compute Engine API access confirmed\")\n    \n    # Test storage API\n    storage_client = storage.Client()\n    print(\"Cloud Storage API access confirmed\")\n    \nexcept Exception as e:\n    print(f\"GCP authentication failed: {e}\")"
  },
  {
   "cell_type": "markdown",
   "id": "eegtd4c0im",
   "source": "**Make sure you have set up authentication and enabled required APIs.**",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "compute-engine-setup",
   "metadata": {},
   "source": [
    "## Method 1: Google Compute Engine Configuration\n",
    "\n",
    "### Create Compute Engine Instance for Clustrix"
   ]
  },
  {
   "cell_type": "code",
   "id": "compute-engine-creation",
   "metadata": {},
   "outputs": [],
   "source": "def create_clustrix_compute_instance(project_id, zone='us-central1-a', machine_type='e2-standard-4'):\n    \"\"\"\n    Create a GCP Compute Engine instance configured for Clustrix.\n    \n    Args:\n        project_id: GCP project ID\n        zone: GCP zone for the instance\n        machine_type: Machine type (CPU/memory configuration)\n    \n    Returns:\n        Instance configuration and gcloud commands\n    \"\"\"\n    \n    # Startup script for instance initialization\n    startup_script = '''\n#!/bin/bash\n\n# Update system\napt-get update\napt-get install -y python3 python3-pip git htop curl\n\n# Install clustrix and common packages\npip3 install clustrix numpy scipy pandas scikit-learn matplotlib\n\n# Install uv for faster package management\ncurl -LsSf https://astral.sh/uv/install.sh | sh\nsource ~/.cargo/env\n\n# Create clustrix user\nuseradd -m -s /bin/bash clustrix\nusermod -aG sudo clustrix\necho \"clustrix ALL=(ALL) NOPASSWD:ALL\" >> /etc/sudoers\n\n# Setup SSH for clustrix user\nmkdir -p /home/clustrix/.ssh\n# Copy SSH keys from default user\nif [ -d \"/home/$(logname)/.ssh\" ]; then\n    cp -r /home/$(logname)/.ssh/* /home/clustrix/.ssh/\n    chown -R clustrix:clustrix /home/clustrix/.ssh\n    chmod 700 /home/clustrix/.ssh\n    chmod 600 /home/clustrix/.ssh/authorized_keys 2>/dev/null || true\nfi\n\n# Create working directory\nmkdir -p /tmp/clustrix\nchown clustrix:clustrix /tmp/clustrix\n\n# Install Google Cloud SDK for clustrix user\ncurl https://sdk.cloud.google.com | bash\nexec -l $SHELL\n\n# Log completion\necho \"Clustrix setup completed at $(date)\" >> /var/log/clustrix-setup.log\n'''\n    \n    # gcloud commands for instance creation\n    gcloud_commands = f\"\"\"\n# Create firewall rule for SSH (if not exists)\ngcloud compute firewall-rules create allow-ssh \\\n  --allow tcp:22 \\\n  --source-ranges 0.0.0.0/0 \\\n  --description \"Allow SSH access\" \\\n  --project {project_id} || echo \"SSH rule already exists\"\n\n# Create the instance\ngcloud compute instances create clustrix-instance \\\n  --project={project_id} \\\n  --zone={zone} \\\n  --machine-type={machine_type} \\\n  --network-interface=network-tier=PREMIUM,subnet=default \\\n  --maintenance-policy=MIGRATE \\\n  --provisioning-model=STANDARD \\\n  --service-account=default \\\n  --scopes=https://www.googleapis.com/auth/cloud-platform \\\n  --tags=clustrix,http-server,https-server \\\n  --create-disk=auto-delete=yes,boot=yes,device-name=clustrix-instance,image=projects/ubuntu-os-cloud/global/images/family/ubuntu-2204-lts,mode=rw,size=50,type=projects/{project_id}/zones/{zone}/diskTypes/pd-balanced \\\n  --no-shielded-secure-boot \\\n  --shielded-vtpm \\\n  --shielded-integrity-monitoring \\\n  --labels=purpose=clustrix,environment=tutorial \\\n  --reservation-affinity=any \\\n  --metadata-from-file startup-script=startup-script.sh\n\n# Get the external IP\ngcloud compute instances describe clustrix-instance \\\n  --project={project_id} \\\n  --zone={zone} \\\n  --format='get(networkInterfaces[0].accessConfigs[0].natIP)'\n\n# SSH to the instance (after startup script completes)\ngcloud compute ssh clustrix-instance \\\n  --project={project_id} \\\n  --zone={zone}\n\"\"\"\n    \n    return {\n        'project_id': project_id,\n        'zone': zone,\n        'machine_type': machine_type,\n        'instance_name': 'clustrix-instance',\n        'gcloud_commands': gcloud_commands,\n        'startup_script': startup_script\n    }\n\n# Example usage\ninstance_config = create_clustrix_compute_instance(\n    project_id='your-project-id',  # Replace with your project ID\n    zone='us-central1-a',\n    machine_type='e2-standard-4'  # 4 vCPUs, 16 GB RAM\n)\n\nprint(\"GCP Compute Engine Instance Creation:\")\nprint(instance_config['gcloud_commands'])\nprint(\"\\nStartup Script:\")\nprint(instance_config['startup_script'])"
  },
  {
   "cell_type": "markdown",
   "id": "nrfay0eolc",
   "source": "**Save the startup script and execute the gcloud commands to create your instance.**",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "clustrix-gcp-config",
   "metadata": {},
   "source": [
    "### Configure Clustrix for Compute Engine"
   ]
  },
  {
   "cell_type": "code",
   "id": "config-gcp-compute",
   "metadata": {},
   "outputs": [],
   "source": "# Configure Clustrix to use your Compute Engine instance\nconfigure(\n    cluster_type=\"ssh\",\n    cluster_host=\"your-instance-external-ip\",  # Replace with actual IP\n    username=\"clustrix\",  # or your default user\n    key_file=\"~/.ssh/google_compute_engine\",  # gcloud-generated key\n    remote_work_dir=\"/tmp/clustrix\",\n    package_manager=\"auto\",  # Will use uv if available\n    default_cores=4,\n    default_memory=\"8GB\",\n    default_time=\"01:00:00\"\n)"
  },
  {
   "cell_type": "markdown",
   "id": "6qtk506dlio",
   "source": "**Replace `your-instance-external-ip` with the actual external IP from your Compute Engine instance.**",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "gcp-example",
   "metadata": {},
   "source": [
    "### Example: Remote Computation on Compute Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gcp-compute-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cluster(cores=2, memory=\"4GB\")\n",
    "def gcp_data_analysis(dataset_size=10000, analysis_type='regression'):\n",
    "    \"\"\"Perform data analysis on GCP Compute Engine.\"\"\"\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "    from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "    from sklearn.datasets import make_regression, make_classification\n",
    "    import time\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Generate synthetic dataset\n",
    "    if analysis_type == 'regression':\n",
    "        X, y = make_regression(\n",
    "            n_samples=dataset_size,\n",
    "            n_features=20,\n",
    "            noise=0.1,\n",
    "            random_state=42\n",
    "        )\n",
    "        model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "        metric_name = 'rmse'\n",
    "    else:\n",
    "        X, y = make_classification(\n",
    "            n_samples=dataset_size,\n",
    "            n_features=20,\n",
    "            n_classes=3,\n",
    "            random_state=42\n",
    "        )\n",
    "        model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "        metric_name = 'accuracy'\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    training_start = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    training_time = time.time() - training_start\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    if analysis_type == 'regression':\n",
    "        metric_value = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    else:\n",
    "        metric_value = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    return {\n",
    "        'analysis_type': analysis_type,\n",
    "        'dataset_size': dataset_size,\n",
    "        'training_time': training_time,\n",
    "        'total_time': total_time,\n",
    "        metric_name: metric_value,\n",
    "        'feature_importance': model.feature_importances_[:5].tolist(),  # Top 5\n",
    "        'training_samples': len(X_train),\n",
    "        'test_samples': len(X_test)\n",
    "    }\n",
    "\n",
    "# Run computation on GCP Compute Engine\n",
    "# result = gcp_data_analysis(dataset_size=50000, analysis_type='classification')\n",
    "# print(f\"Analysis completed: {result['accuracy']:.4f} accuracy in {result['total_time']:.2f} seconds\")\n",
    "print(\"Example function defined. Uncomment the lines above to run on your GCP instance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gke-setup",
   "metadata": {},
   "source": [
    "## Method 2: Google Kubernetes Engine (GKE) Configuration\n",
    "\n",
    "GKE provides managed Kubernetes clusters ideal for containerized Clustrix workloads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gke-cluster-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_gke_cluster_for_clustrix(project_id, cluster_name='clustrix-cluster', zone='us-central1-a'):\n",
    "    \"\"\"\n",
    "    Setup GKE cluster optimized for Clustrix workloads.\n",
    "    \"\"\"\n",
    "    \n",
    "    gke_commands = f\"\"\"\n",
    "# Enable required APIs\n",
    "gcloud services enable container.googleapis.com \\\n",
    "  --project {project_id}\n",
    "\n",
    "# Create GKE cluster with auto-scaling\n",
    "gcloud container clusters create {cluster_name} \\\n",
    "  --project {project_id} \\\n",
    "  --zone {zone} \\\n",
    "  --machine-type e2-standard-4 \\\n",
    "  --num-nodes 1 \\\n",
    "  --enable-autoscaling \\\n",
    "  --min-nodes 0 \\\n",
    "  --max-nodes 10 \\\n",
    "  --enable-autorepair \\\n",
    "  --enable-autoupgrade \\\n",
    "  --disk-size 50GB \\\n",
    "  --disk-type pd-ssd \\\n",
    "  --enable-network-policy \\\n",
    "  --enable-ip-alias \\\n",
    "  --labels purpose=clustrix,environment=tutorial\n",
    "\n",
    "# Get cluster credentials\n",
    "gcloud container clusters get-credentials {cluster_name} \\\n",
    "  --project {project_id} \\\n",
    "  --zone {zone}\n",
    "\n",
    "# Verify cluster access\n",
    "kubectl get nodes\n",
    "\n",
    "# Create clustrix namespace\n",
    "kubectl create namespace clustrix\n",
    "\n",
    "# Set as default namespace\n",
    "kubectl config set-context --current --namespace=clustrix\n",
    "\"\"\"\n",
    "    \n",
    "    # Clustrix job template for Kubernetes\n",
    "    k8s_job_template = \"\"\"\n",
    "apiVersion: batch/v1\n",
    "kind: Job\n",
    "metadata:\n",
    "  name: clustrix-job-${JOB_ID}\n",
    "  namespace: clustrix\n",
    "spec:\n",
    "  template:\n",
    "    spec:\n",
    "      restartPolicy: Never\n",
    "      containers:\n",
    "      - name: clustrix-worker\n",
    "        image: python:3.11-slim\n",
    "        command: [\"bash\", \"-c\"]\n",
    "        args:\n",
    "        - |\n",
    "          pip install clustrix numpy scipy pandas scikit-learn\n",
    "          python -c \"\n",
    "          import pickle\n",
    "          import sys\n",
    "          \n",
    "          # Load and execute function\n",
    "          with open('/data/function_data.pkl', 'rb') as f:\n",
    "              data = pickle.load(f)\n",
    "          \n",
    "          func = pickle.loads(data['function'])\n",
    "          args = pickle.loads(data['args'])\n",
    "          kwargs = pickle.loads(data['kwargs'])\n",
    "          \n",
    "          try:\n",
    "              result = func(*args, **kwargs)\n",
    "              with open('/data/result.pkl', 'wb') as f:\n",
    "                  pickle.dump(result, f)\n",
    "          except Exception as e:\n",
    "              with open('/data/error.pkl', 'wb') as f:\n",
    "                  pickle.dump({'error': str(e)}, f)\n",
    "              raise\n",
    "          \"\n",
    "        resources:\n",
    "          requests:\n",
    "            memory: \"2Gi\"\n",
    "            cpu: \"1\"\n",
    "          limits:\n",
    "            memory: \"4Gi\"\n",
    "            cpu: \"2\"\n",
    "        volumeMounts:\n",
    "        - name: job-data\n",
    "          mountPath: /data\n",
    "      volumes:\n",
    "      - name: job-data\n",
    "        persistentVolumeClaim:\n",
    "          claimName: clustrix-pvc\n",
    "  backoffLimit: 3\n",
    "\"\"\"\n",
    "    \n",
    "    print(\"GKE Cluster Setup Commands:\")\n",
    "    print(gke_commands)\n",
    "    print(\"\\nKubernetes Job Template:\")\n",
    "    print(k8s_job_template)\n",
    "    \n",
    "    return {\n",
    "        'cluster_name': cluster_name,\n",
    "        'project_id': project_id,\n",
    "        'zone': zone,\n",
    "        'setup_commands': gke_commands,\n",
    "        'job_template': k8s_job_template\n",
    "    }\n",
    "\n",
    "def configure_clustrix_for_gke(cluster_endpoint, cluster_name):\n",
    "    \"\"\"Configure Clustrix to use GKE cluster.\"\"\"\n",
    "    configure(\n",
    "        cluster_type=\"kubernetes\",\n",
    "        cluster_host=cluster_endpoint,\n",
    "        # For GKE, authentication is handled via kubectl config\n",
    "        remote_work_dir=\"/tmp/clustrix\",\n",
    "        package_manager=\"pip\",  # Container-based, pip is fine\n",
    "        default_cores=2,\n",
    "        default_memory=\"4GB\",\n",
    "        default_time=\"01:00:00\"\n",
    "    )\n",
    "    print(f\"Configured Clustrix for GKE cluster: {cluster_name}\")\n",
    "\n",
    "gke_config = setup_gke_cluster_for_clustrix(\n",
    "    project_id='your-project-id',\n",
    "    cluster_name='clustrix-cluster'\n",
    ")\n",
    "\n",
    "print(\"\\nNote: GKE integration requires additional implementation in Clustrix.\")\n",
    "print(\"Current Clustrix supports basic Kubernetes, but GKE-specific features need custom setup.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gcp-batch",
   "metadata": {},
   "source": [
    "## Method 3: Google Cloud Batch\n",
    "\n",
    "Google Cloud Batch provides managed job scheduling for large-scale workloads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gcp-batch-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_gcp_batch_environment(project_id, region='us-central1'):\n",
    "    \"\"\"\n",
    "    Setup Google Cloud Batch for Clustrix workloads.\n",
    "    \"\"\"\n",
    "    \n",
    "    batch_setup_commands = f\"\"\"\n",
    "# Enable Batch API\n",
    "gcloud services enable batch.googleapis.com \\\n",
    "  --project {project_id}\n",
    "\n",
    "# Create a service account for Batch jobs\n",
    "gcloud iam service-accounts create clustrix-batch-sa \\\n",
    "  --project {project_id} \\\n",
    "  --description=\"Service account for Clustrix Batch jobs\" \\\n",
    "  --display-name=\"Clustrix Batch Service Account\"\n",
    "\n",
    "# Grant necessary permissions\n",
    "gcloud projects add-iam-policy-binding {project_id} \\\n",
    "  --member=\"serviceAccount:clustrix-batch-sa@{project_id}.iam.gserviceaccount.com\" \\\n",
    "  --role=\"roles/batch.jobsEditor\"\n",
    "\n",
    "gcloud projects add-iam-policy-binding {project_id} \\\n",
    "  --member=\"serviceAccount:clustrix-batch-sa@{project_id}.iam.gserviceaccount.com\" \\\n",
    "  --role=\"roles/storage.objectAdmin\"\n",
    "\n",
    "# Create Cloud Storage bucket for job data\n",
    "gsutil mb -p {project_id} -l {region} gs://{project_id}-clustrix-batch\n",
    "\"\"\"\n",
    "    \n",
    "    # Batch job configuration template\n",
    "    batch_job_config = {\n",
    "        \"taskGroups\": [\n",
    "            {\n",
    "                \"taskSpec\": {\n",
    "                    \"runnables\": [\n",
    "                        {\n",
    "                            \"script\": {\n",
    "                                \"text\": \"\"\"\n",
    "#!/bin/bash\n",
    "set -e\n",
    "\n",
    "# Install required packages\n",
    "pip3 install clustrix numpy scipy pandas scikit-learn\n",
    "\n",
    "# Download job data from Cloud Storage\n",
    "gsutil cp gs://{project_id}-clustrix-batch/jobs/${{BATCH_JOB_ID}}/function_data.pkl .\n",
    "\n",
    "# Execute the function\n",
    "python3 -c \"\n",
    "import pickle\n",
    "import traceback\n",
    "\n",
    "try:\n",
    "    with open('function_data.pkl', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    func = pickle.loads(data['function'])\n",
    "    args = pickle.loads(data['args'])\n",
    "    kwargs = pickle.loads(data['kwargs'])\n",
    "    \n",
    "    result = func(*args, **kwargs)\n",
    "    \n",
    "    with open('result.pkl', 'wb') as f:\n",
    "        pickle.dump(result, f)\n",
    "        \n",
    "except Exception as e:\n",
    "    with open('error.pkl', 'wb') as f:\n",
    "        pickle.dump({{\n",
    "            'error': str(e),\n",
    "            'traceback': traceback.format_exc()\n",
    "        }}, f)\n",
    "    raise\n",
    "\"\n",
    "\n",
    "# Upload results to Cloud Storage\n",
    "gsutil cp result.pkl gs://{project_id}-clustrix-batch/jobs/${{BATCH_JOB_ID}}/result.pkl || \\\n",
    "gsutil cp error.pkl gs://{project_id}-clustrix-batch/jobs/${{BATCH_JOB_ID}}/error.pkl\n",
    "\"\"\"\n",
    "                            }\n",
    "                        }\n",
    "                    ],\n",
    "                    \"computeResource\": {\n",
    "                        \"cpuMilli\": 2000,  # 2 CPUs\n",
    "                        \"memoryMib\": 4096  # 4 GB RAM\n",
    "                    },\n",
    "                    \"maxRetryCount\": 2,\n",
    "                    \"maxRunDuration\": \"3600s\"  # 1 hour\n",
    "                },\n",
    "                \"taskCount\": 1\n",
    "            }\n",
    "        ],\n",
    "        \"allocationPolicy\": {\n",
    "            \"instances\": [\n",
    "                {\n",
    "                    \"instanceTemplate\": {\n",
    "                        \"machineType\": \"e2-standard-2\",\n",
    "                        \"provisioningModel\": \"STANDARD\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        \"labels\": {\n",
    "            \"purpose\": \"clustrix\",\n",
    "            \"environment\": \"tutorial\"\n",
    "        },\n",
    "        \"logsPolicy\": {\n",
    "            \"destination\": \"CLOUD_LOGGING\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"Google Cloud Batch Setup Commands:\")\n",
    "    print(batch_setup_commands)\n",
    "    print(\"\\nBatch Job Configuration:\")\n",
    "    print(json.dumps(batch_job_config, indent=2))\n",
    "    \n",
    "    return {\n",
    "        'project_id': project_id,\n",
    "        'region': region,\n",
    "        'bucket_name': f'{project_id}-clustrix-batch',\n",
    "        'service_account': f'clustrix-batch-sa@{project_id}.iam.gserviceaccount.com',\n",
    "        'job_config': batch_job_config,\n",
    "        'setup_commands': batch_setup_commands\n",
    "    }\n",
    "\n",
    "batch_config = setup_gcp_batch_environment('your-project-id')\n",
    "print(\"\\nGoogle Cloud Batch provides excellent integration for large-scale Clustrix workloads.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cloud-storage",
   "metadata": {},
   "source": [
    "## Data Management with Google Cloud Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cloud-storage-integration",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cluster(cores=2, memory=\"4GB\")\n",
    "def process_gcs_data(bucket_name, input_blob, output_blob, project_id=None):\n",
    "    \"\"\"Process data from Google Cloud Storage and save results back.\"\"\"\n",
    "    from google.cloud import storage\n",
    "    import numpy as np\n",
    "    import pickle\n",
    "    import io\n",
    "    import time\n",
    "    \n",
    "    # Initialize Cloud Storage client\n",
    "    storage_client = storage.Client(project=project_id)\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    \n",
    "    # Download data from Cloud Storage\n",
    "    input_blob_obj = bucket.blob(input_blob)\n",
    "    data_bytes = input_blob_obj.download_as_bytes()\n",
    "    data = pickle.loads(data_bytes)\n",
    "    \n",
    "    # Process the data\n",
    "    processed_data = {\n",
    "        'original_shape': data.shape if hasattr(data, 'shape') else len(data) if hasattr(data, '__len__') else 'scalar',\n",
    "        'mean': float(np.mean(data)) if hasattr(data, '__iter__') else float(data),\n",
    "        'std': float(np.std(data)) if hasattr(data, '__iter__') else 0.0,\n",
    "        'max': float(np.max(data)) if hasattr(data, '__iter__') else float(data),\n",
    "        'min': float(np.min(data)) if hasattr(data, '__iter__') else float(data),\n",
    "        'processing_timestamp': time.time(),\n",
    "        'processed_on': 'gcp-compute-engine',\n",
    "        'data_type': str(type(data).__name__)\n",
    "    }\n",
    "    \n",
    "    # Advanced processing based on data type\n",
    "    if hasattr(data, 'shape') and len(data.shape) >= 2:\n",
    "        # Matrix operations\n",
    "        processed_data.update({\n",
    "            'matrix_rank': int(np.linalg.matrix_rank(data)) if data.shape[0] == data.shape[1] else 'non_square',\n",
    "            'frobenius_norm': float(np.linalg.norm(data, 'fro')),\n",
    "            'condition_number': float(np.linalg.cond(data)) if data.shape[0] == data.shape[1] else None\n",
    "        })\n",
    "    \n",
    "    # Upload results to Cloud Storage\n",
    "    output_bytes = pickle.dumps(processed_data)\n",
    "    output_blob_obj = bucket.blob(output_blob)\n",
    "    output_blob_obj.upload_from_string(output_bytes)\n",
    "    \n",
    "    return f\"Processed data saved to gs://{bucket_name}/{output_blob}\"\n",
    "\n",
    "# Utility functions for Google Cloud Storage\n",
    "def upload_to_gcs(data, bucket_name, blob_name, project_id=None):\n",
    "    \"\"\"Upload data to Google Cloud Storage.\"\"\"\n",
    "    storage_client = storage.Client(project=project_id)\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "    \n",
    "    data_bytes = pickle.dumps(data)\n",
    "    blob.upload_from_string(data_bytes)\n",
    "    print(f\"Data uploaded to gs://{bucket_name}/{blob_name}\")\n",
    "\n",
    "def download_from_gcs(bucket_name, blob_name, project_id=None):\n",
    "    \"\"\"Download data from Google Cloud Storage.\"\"\"\n",
    "    storage_client = storage.Client(project=project_id)\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "    \n",
    "    data_bytes = blob.download_as_bytes()\n",
    "    return pickle.loads(data_bytes)\n",
    "\n",
    "def create_gcs_bucket_for_clustrix(project_id, bucket_name, location='us-central1'):\n",
    "    \"\"\"Create a Cloud Storage bucket for Clustrix data.\"\"\"\n",
    "    gcs_commands = f\"\"\"\n",
    "# Create bucket with appropriate settings\n",
    "gsutil mb -p {project_id} -l {location} gs://{bucket_name}\n",
    "\n",
    "# Set lifecycle policy to delete temporary files after 7 days\n",
    "echo '{{\n",
    "  \"lifecycle\": {{\n",
    "    \"rule\": [\n",
    "      {{\n",
    "        \"action\": {{\"type\": \"Delete\"}},\n",
    "        \"condition\": {{\n",
    "          \"age\": 7,\n",
    "          \"matchesPrefix\": [\"temp/\"]\n",
    "        }}\n",
    "      }}\n",
    "    ]\n",
    "  }}\n",
    "}}' > lifecycle.json\n",
    "\n",
    "gsutil lifecycle set lifecycle.json gs://{bucket_name}\n",
    "\n",
    "# Set up proper permissions (if using service account)\n",
    "gsutil iam ch serviceAccount:clustrix-batch-sa@{project_id}.iam.gserviceaccount.com:objectAdmin gs://{bucket_name}\n",
    "\"\"\"\n",
    "    \n",
    "    print(f\"Commands to create bucket gs://{bucket_name}:\")\n",
    "    print(gcs_commands)\n",
    "    return gcs_commands\n",
    "\n",
    "# Example usage:\n",
    "# Create bucket first\n",
    "bucket_commands = create_gcs_bucket_for_clustrix('your-project-id', 'your-clustrix-data')\n",
    "\n",
    "# Then use the functions\n",
    "# sample_data = np.random.rand(1000, 100)\n",
    "# upload_to_gcs(sample_data, 'your-clustrix-data', 'input/sample_data.pkl', 'your-project-id')\n",
    "# result = process_gcs_data('your-clustrix-data', 'input/sample_data.pkl', 'output/results.pkl', 'your-project-id')\n",
    "print(\"\\nGoogle Cloud Storage integration functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vertex-ai",
   "metadata": {},
   "source": [
    "## Vertex AI Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vertex-ai-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_vertex_ai_for_clustrix(project_id, region='us-central1'):\n",
    "    \"\"\"\n",
    "    Setup Vertex AI for ML workloads with Clustrix.\n",
    "    \"\"\"\n",
    "    \n",
    "    vertex_commands = f\"\"\"\n",
    "# Enable Vertex AI API\n",
    "gcloud services enable aiplatform.googleapis.com \\\n",
    "  --project {project_id}\n",
    "\n",
    "# Create Vertex AI custom training job\n",
    "gcloud ai custom-jobs create \\\n",
    "  --region={region} \\\n",
    "  --display-name=clustrix-training-job \\\n",
    "  --config=training_job_config.yaml\n",
    "\n",
    "# Create Vertex AI endpoints for model serving\n",
    "gcloud ai endpoints create \\\n",
    "  --region={region} \\\n",
    "  --display-name=clustrix-model-endpoint\n",
    "\"\"\"\n",
    "    \n",
    "    # Vertex AI training job configuration\n",
    "    training_config = \"\"\"\n",
    "# training_job_config.yaml\n",
    "workerPoolSpecs:\n",
    "- machineSpec:\n",
    "    machineType: e2-standard-4\n",
    "  replicaCount: 1\n",
    "  containerSpec:\n",
    "    imageUri: gcr.io/cloud-aiplatform/training/tf-cpu.2-8:latest\n",
    "    command:\n",
    "    - python3\n",
    "    - -c\n",
    "    args:\n",
    "    - |\n",
    "      import subprocess\n",
    "      import sys\n",
    "      \n",
    "      # Install clustrix\n",
    "      subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'clustrix', 'numpy', 'pandas', 'scikit-learn'])\n",
    "      \n",
    "      # Your training code here\n",
    "      print(\"Clustrix training job completed on Vertex AI\")\n",
    "    env:\n",
    "    - name: GOOGLE_CLOUD_PROJECT\n",
    "      value: {project_id}\n",
    "    - name: AIP_MODEL_DIR\n",
    "      value: gs://{project_id}-vertex-models\n",
    "\"\"\"\n",
    "    \n",
    "    print(\"Vertex AI Setup Commands:\")\n",
    "    print(vertex_commands)\n",
    "    print(\"\\nTraining Job Configuration:\")\n",
    "    print(training_config)\n",
    "    \n",
    "    return {\n",
    "        'project_id': project_id,\n",
    "        'region': region,\n",
    "        'setup_commands': vertex_commands,\n",
    "        'training_config': training_config\n",
    "    }\n",
    "\n",
    "@cluster(cores=4, memory=\"8GB\")\n",
    "def vertex_ai_ml_pipeline(dataset_config, model_config, project_id, bucket_name):\n",
    "    \"\"\"ML pipeline that could run on Vertex AI with Clustrix.\"\"\"\n",
    "    import numpy as np\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "    from sklearn.datasets import make_classification\n",
    "    from sklearn.metrics import classification_report\n",
    "    from google.cloud import storage\n",
    "    import pickle\n",
    "    import time\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Generate or load dataset\n",
    "    X, y = make_classification(\n",
    "        n_samples=dataset_config['n_samples'],\n",
    "        n_features=dataset_config['n_features'],\n",
    "        n_classes=dataset_config['n_classes'],\n",
    "        n_informative=dataset_config.get('n_informative', dataset_config['n_features'] // 2),\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Hyperparameter tuning\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.1, 0.2]\n",
    "    }\n",
    "    \n",
    "    # Grid search with cross-validation\n",
    "    model = GradientBoostingClassifier(random_state=42)\n",
    "    grid_search = GridSearchCV(\n",
    "        model, param_grid, cv=5, scoring='accuracy', n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X, y)\n",
    "    \n",
    "    # Get best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    # Evaluate with cross-validation\n",
    "    cv_scores = cross_val_score(best_model, X, y, cv=5, scoring='accuracy')\n",
    "    \n",
    "    # Save model to Cloud Storage\n",
    "    storage_client = storage.Client(project=project_id)\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    \n",
    "    model_blob = bucket.blob('models/clustrix_model.pkl')\n",
    "    model_bytes = pickle.dumps(best_model)\n",
    "    model_blob.upload_from_string(model_bytes)\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    return {\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'best_score': grid_search.best_score_,\n",
    "        'cv_mean_score': cv_scores.mean(),\n",
    "        'cv_std_score': cv_scores.std(),\n",
    "        'training_time': total_time,\n",
    "        'model_location': f'gs://{bucket_name}/models/clustrix_model.pkl',\n",
    "        'feature_importance': best_model.feature_importances_[:10].tolist(),  # Top 10\n",
    "        'dataset_size': len(X)\n",
    "    }\n",
    "\n",
    "vertex_config = setup_vertex_ai_for_clustrix('your-project-id')\n",
    "\n",
    "# Example usage:\n",
    "# dataset_params = {'n_samples': 10000, 'n_features': 20, 'n_classes': 3}\n",
    "# model_params = {}\n",
    "# result = vertex_ai_ml_pipeline(dataset_params, model_params, 'your-project-id', 'your-bucket')\n",
    "# print(f\"Best model score: {result['best_score']:.4f}\")\n",
    "\n",
    "print(\"Vertex AI integration examples defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preemptible-vms",
   "metadata": {},
   "source": [
    "## Cost Optimization with Preemptible VMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preemptible-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_preemptible_cluster(project_id, zone='us-central1-a'):\n",
    "    \"\"\"\n",
    "    Setup cost-effective cluster using preemptible VMs.\n",
    "    \"\"\"\n",
    "    \n",
    "    preemptible_commands = f\"\"\"\n",
    "# Create preemptible instance template\n",
    "gcloud compute instance-templates create clustrix-preemptible-template \\\n",
    "  --project {project_id} \\\n",
    "  --machine-type e2-standard-4 \\\n",
    "  --preemptible \\\n",
    "  --boot-disk-size 50GB \\\n",
    "  --boot-disk-type pd-standard \\\n",
    "  --image-family ubuntu-2204-lts \\\n",
    "  --image-project ubuntu-os-cloud \\\n",
    "  --metadata-from-file startup-script=clustrix-startup.sh \\\n",
    "  --scopes cloud-platform \\\n",
    "  --tags clustrix,preemptible\n",
    "\n",
    "# Create managed instance group\n",
    "gcloud compute instance-groups managed create clustrix-preemptible-group \\\n",
    "  --project {project_id} \\\n",
    "  --zone {zone} \\\n",
    "  --template clustrix-preemptible-template \\\n",
    "  --size 0\n",
    "\n",
    "# Set up auto-scaling\n",
    "gcloud compute instance-groups managed set-autoscaling clustrix-preemptible-group \\\n",
    "  --project {project_id} \\\n",
    "  --zone {zone} \\\n",
    "  --min-num-replicas 0 \\\n",
    "  --max-num-replicas 10 \\\n",
    "  --target-cpu-utilization 0.6\n",
    "\n",
    "# Scale up the group\n",
    "gcloud compute instance-groups managed resize clustrix-preemptible-group \\\n",
    "  --project {project_id} \\\n",
    "  --zone {zone} \\\n",
    "  --size 2\n",
    "\"\"\"\n",
    "    \n",
    "    cost_optimization_tips = \"\"\"\n",
    "GCP Cost Optimization for Clustrix:\n",
    "\n",
    "1. Compute Optimization:\n",
    "   - Use Preemptible VMs for fault-tolerant workloads (up to 80% savings)\n",
    "   - Use Spot VMs (successor to Preemptible) for even better savings\n",
    "   - Choose appropriate machine types (E2, N2, C2 based on workload)\n",
    "   - Use sustained use discounts for long-running workloads\n",
    "   - Consider committed use discounts for predictable usage\n",
    "\n",
    "2. Storage Optimization:\n",
    "   - Use appropriate storage classes (Standard, Nearline, Coldline, Archive)\n",
    "   - Enable object lifecycle management\n",
    "   - Use regional storage for better performance/cost balance\n",
    "   - Implement data compression and deduplication\n",
    "\n",
    "3. Network Optimization:\n",
    "   - Minimize inter-region data transfer\n",
    "   - Use Cloud CDN for static content\n",
    "   - Optimize data transfer patterns\n",
    "\n",
    "4. Monitoring and Management:\n",
    "   - Set up budget alerts and quotas\n",
    "   - Use Cloud Billing reports\n",
    "   - Implement proper resource labeling\n",
    "   - Regular cost reviews and right-sizing\n",
    "\n",
    "5. Service-Specific:\n",
    "   - Use Cloud Functions for event-driven tasks\n",
    "   - Consider Cloud Run for containerized applications\n",
    "   - Use Google Cloud Batch for large batch processing\n",
    "\"\"\"\n",
    "    \n",
    "    print(\"Preemptible VM Setup Commands:\")\n",
    "    print(preemptible_commands)\n",
    "    print(\"\\nCost Optimization Tips:\")\n",
    "    print(cost_optimization_tips)\n",
    "    \n",
    "    return {\n",
    "        'project_id': project_id,\n",
    "        'zone': zone,\n",
    "        'template_name': 'clustrix-preemptible-template',\n",
    "        'group_name': 'clustrix-preemptible-group',\n",
    "        'setup_commands': preemptible_commands\n",
    "    }\n",
    "\n",
    "def configure_for_preemptible():\n",
    "    \"\"\"Configure Clustrix for preemptible VM usage.\"\"\"\n",
    "    configure(\n",
    "        cluster_type=\"ssh\",\n",
    "        cluster_host=\"preemptible-instance-ip\",\n",
    "        username=\"clustrix\",\n",
    "        key_file=\"~/.ssh/google_compute_engine\",\n",
    "        remote_work_dir=\"/tmp/clustrix\",\n",
    "        package_manager=\"uv\",\n",
    "        # Preemptible VMs can be terminated, use shorter timeouts\n",
    "        default_time=\"00:30:00\",\n",
    "        job_poll_interval=30,  # Check more frequently\n",
    "        cleanup_on_success=True,  # Clean up quickly\n",
    "        # Save work frequently\n",
    "        max_parallel_jobs=20  # Higher parallelism for fault tolerance\n",
    "    )\n",
    "    print(\"Configured Clustrix for preemptible VMs with fault-tolerant settings.\")\n",
    "\n",
    "preemptible_config = setup_preemptible_cluster('your-project-id')\n",
    "print(\"\\nPreemptible VMs can provide up to 80% cost savings for fault-tolerant workloads.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gcp-security",
   "metadata": {},
   "source": [
    "## Security Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gcp-security-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_gcp_security_for_clustrix(project_id):\n",
    "    \"\"\"\n",
    "    Security configuration for GCP + Clustrix deployment.\n",
    "    \"\"\"\n",
    "    \n",
    "    security_commands = f\"\"\"\n",
    "# Create VPC with private subnets\n",
    "gcloud compute networks create clustrix-vpc \\\n",
    "  --project {project_id} \\\n",
    "  --subnet-mode custom\n",
    "\n",
    "gcloud compute networks subnets create clustrix-subnet \\\n",
    "  --project {project_id} \\\n",
    "  --network clustrix-vpc \\\n",
    "  --range 10.1.0.0/24 \\\n",
    "  --region us-central1 \\\n",
    "  --enable-private-ip-google-access\n",
    "\n",
    "# Create firewall rules (restrictive)\n",
    "gcloud compute firewall-rules create clustrix-allow-ssh \\\n",
    "  --project {project_id} \\\n",
    "  --network clustrix-vpc \\\n",
    "  --allow tcp:22 \\\n",
    "  --source-ranges YOUR_IP/32 \\\n",
    "  --target-tags clustrix\n",
    "\n",
    "gcloud compute firewall-rules create clustrix-internal \\\n",
    "  --project {project_id} \\\n",
    "  --network clustrix-vpc \\\n",
    "  --allow tcp,udp,icmp \\\n",
    "  --source-ranges 10.1.0.0/24 \\\n",
    "  --target-tags clustrix\n",
    "\n",
    "# Create service account with minimal permissions\n",
    "gcloud iam service-accounts create clustrix-compute \\\n",
    "  --project {project_id} \\\n",
    "  --description=\"Service account for Clustrix compute instances\" \\\n",
    "  --display-name=\"Clustrix Compute Service Account\"\n",
    "\n",
    "# Grant only necessary permissions\n",
    "gcloud projects add-iam-policy-binding {project_id} \\\n",
    "  --member=\"serviceAccount:clustrix-compute@{project_id}.iam.gserviceaccount.com\" \\\n",
    "  --role=\"roles/storage.objectAdmin\"\n",
    "\n",
    "gcloud projects add-iam-policy-binding {project_id} \\\n",
    "  --member=\"serviceAccount:clustrix-compute@{project_id}.iam.gserviceaccount.com\" \\\n",
    "  --role=\"roles/logging.logWriter\"\n",
    "\n",
    "# Enable OS Login for better SSH key management\n",
    "gcloud compute project-info add-metadata \\\n",
    "  --project {project_id} \\\n",
    "  --metadata enable-oslogin=TRUE\n",
    "\n",
    "# Create Cloud KMS key for encryption\n",
    "gcloud kms keyrings create clustrix-keyring \\\n",
    "  --project {project_id} \\\n",
    "  --location global\n",
    "\n",
    "gcloud kms keys create clustrix-key \\\n",
    "  --project {project_id} \\\n",
    "  --keyring clustrix-keyring \\\n",
    "  --location global \\\n",
    "  --purpose encryption\n",
    "\"\"\"\n",
    "    \n",
    "    security_checklist = \"\"\"\n",
    "GCP Security Checklist for Clustrix:\n",
    "\n",
    "‚úì Use IAM service accounts with minimal permissions\n",
    "‚úì Enable OS Login for centralized SSH key management\n",
    "‚úì Create custom VPC with private subnets\n",
    "‚úì Restrict firewall rules to specific IP ranges\n",
    "‚úì Enable private Google access for instances without external IPs\n",
    "‚úì Use Cloud KMS for encryption at rest\n",
    "‚úì Enable audit logging and Cloud Security Command Center\n",
    "‚úì Use Binary Authorization for container security\n",
    "‚úì Implement VPC Service Controls for data perimeter\n",
    "‚úì Enable DDoS protection and Cloud Armor\n",
    "‚úì Use Secret Manager for sensitive configuration\n",
    "‚úì Enable vulnerability scanning for container images\n",
    "‚úì Set up budget alerts and billing account security\n",
    "‚úì Use organization policies for governance\n",
    "‚úì Regular security reviews and access audits\n",
    "\"\"\"\n",
    "    \n",
    "    print(\"GCP Security Setup Commands:\")\n",
    "    print(security_commands)\n",
    "    print(\"\\nSecurity Checklist:\")\n",
    "    print(security_checklist)\n",
    "    \n",
    "    return {\n",
    "        'project_id': project_id,\n",
    "        'vpc_name': 'clustrix-vpc',\n",
    "        'subnet_name': 'clustrix-subnet',\n",
    "        'service_account': f'clustrix-compute@{project_id}.iam.gserviceaccount.com',\n",
    "        'security_commands': security_commands\n",
    "    }\n",
    "\n",
    "security_config = setup_gcp_security_for_clustrix('your-project-id')\n",
    "print(\"Security configuration templates generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup-gcp",
   "metadata": {},
   "source": [
    "## Resource Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup-gcp-resources",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_gcp_resources(project_id, zone='us-central1-a', region='us-central1'):\n",
    "    \"\"\"\n",
    "    Clean up GCP resources to avoid ongoing charges.\n",
    "    \n",
    "    Args:\n",
    "        project_id: GCP project ID\n",
    "        zone: Zone where resources were created\n",
    "        region: Region where resources were created\n",
    "    \"\"\"\n",
    "    \n",
    "    cleanup_commands = f\"\"\"\n",
    "# List all compute instances\n",
    "gcloud compute instances list --project {project_id}\n",
    "\n",
    "# Delete specific instances\n",
    "gcloud compute instances delete clustrix-instance \\\n",
    "  --project {project_id} \\\n",
    "  --zone {zone} \\\n",
    "  --quiet\n",
    "\n",
    "# Delete managed instance groups\n",
    "gcloud compute instance-groups managed delete clustrix-preemptible-group \\\n",
    "  --project {project_id} \\\n",
    "  --zone {zone} \\\n",
    "  --quiet\n",
    "\n",
    "# Delete instance templates\n",
    "gcloud compute instance-templates delete clustrix-preemptible-template \\\n",
    "  --project {project_id} \\\n",
    "  --quiet\n",
    "\n",
    "# Delete GKE clusters\n",
    "gcloud container clusters delete clustrix-cluster \\\n",
    "  --project {project_id} \\\n",
    "  --zone {zone} \\\n",
    "  --quiet\n",
    "\n",
    "# Delete Cloud Storage buckets (BE CAREFUL - THIS DELETES ALL DATA)\n",
    "gsutil -m rm -r gs://{project_id}-clustrix-batch\n",
    "gsutil -m rm -r gs://{project_id}-vertex-models\n",
    "\n",
    "# Delete firewall rules\n",
    "gcloud compute firewall-rules delete clustrix-allow-ssh clustrix-internal \\\n",
    "  --project {project_id} \\\n",
    "  --quiet\n",
    "\n",
    "# Delete VPC network\n",
    "gcloud compute networks subnets delete clustrix-subnet \\\n",
    "  --project {project_id} \\\n",
    "  --region {region} \\\n",
    "  --quiet\n",
    "\n",
    "gcloud compute networks delete clustrix-vpc \\\n",
    "  --project {project_id} \\\n",
    "  --quiet\n",
    "\n",
    "# Delete service accounts\n",
    "gcloud iam service-accounts delete clustrix-compute@{project_id}.iam.gserviceaccount.com \\\n",
    "  --project {project_id} \\\n",
    "  --quiet\n",
    "\n",
    "gcloud iam service-accounts delete clustrix-batch-sa@{project_id}.iam.gserviceaccount.com \\\n",
    "  --project {project_id} \\\n",
    "  --quiet\n",
    "\n",
    "# List remaining billable resources\n",
    "gcloud compute instances list --project {project_id}\n",
    "gcloud compute disks list --project {project_id}\n",
    "gcloud compute addresses list --project {project_id}\n",
    "gcloud container clusters list --project {project_id}\n",
    "\"\"\"\n",
    "    \n",
    "    print(f\"GCP Resource Cleanup Commands for Project: {project_id}\")\n",
    "    print(cleanup_commands)\n",
    "    print(\"\\n‚ö†Ô∏è  WARNING: Some commands will permanently delete resources and data!\")\n",
    "    print(\"Review each resource before deleting and ensure you have backups if needed.\")\n",
    "    print(\"\\nüí° TIP: Use 'gcloud compute instances stop' instead of 'delete' to preserve instances while stopping charges.\")\n",
    "    \n",
    "    return {\n",
    "        'project_id': project_id,\n",
    "        'zone': zone,\n",
    "        'region': region,\n",
    "        'cleanup_commands': cleanup_commands\n",
    "    }\n",
    "\n",
    "cleanup_info = cleanup_gcp_resources('your-project-id')\n",
    "print(\"\\nCleanup commands generated. Always verify resources before deletion!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-gcp-example",
   "metadata": {},
   "source": [
    "## Advanced Example: Distributed Scientific Computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scientific-computing-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cluster(cores=4, memory=\"8GB\", time=\"01:00:00\")\n",
    "def gcp_scientific_simulation(simulation_params, storage_config):\n",
    "    \"\"\"\n",
    "    Distributed scientific simulation using GCP infrastructure.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from scipy.integrate import odeint\n",
    "    from scipy.optimize import minimize\n",
    "    from google.cloud import storage\n",
    "    import pickle\n",
    "    import time\n",
    "    import matplotlib.pyplot as plt\n",
    "    import io\n",
    "    \n",
    "    def lorenz_system(state, t, sigma, rho, beta):\n",
    "        \"\"\"Lorenz attractor differential equations.\"\"\"\n",
    "        x, y, z = state\n",
    "        return [\n",
    "            sigma * (y - x),\n",
    "            x * (rho - z) - y,\n",
    "            x * y - beta * z\n",
    "        ]\n",
    "    \n",
    "    def simulate_lorenz(params, time_points):\n",
    "        \"\"\"Simulate Lorenz system with given parameters.\"\"\"\n",
    "        initial_state = [1.0, 1.0, 1.0]\n",
    "        solution = odeint(\n",
    "            lorenz_system, initial_state, time_points,\n",
    "            args=(params['sigma'], params['rho'], params['beta'])\n",
    "        )\n",
    "        return solution\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Parameter sweep\n",
    "    parameter_sets = simulation_params['parameter_sets']\n",
    "    time_points = np.linspace(0, simulation_params['max_time'], simulation_params['num_points'])\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, params in enumerate(parameter_sets):\n",
    "        # Run simulation\n",
    "        solution = simulate_lorenz(params, time_points)\n",
    "        \n",
    "        # Analyze results\n",
    "        x, y, z = solution[:, 0], solution[:, 1], solution[:, 2]\n",
    "        \n",
    "        analysis = {\n",
    "            'params': params,\n",
    "            'max_x': float(np.max(x)),\n",
    "            'min_x': float(np.min(x)),\n",
    "            'max_y': float(np.max(y)),\n",
    "            'min_y': float(np.min(y)),\n",
    "            'max_z': float(np.max(z)),\n",
    "            'min_z': float(np.min(z)),\n",
    "            'mean_energy': float(np.mean(x**2 + y**2 + z**2)),\n",
    "            'final_state': [float(x[-1]), float(y[-1]), float(z[-1])]\n",
    "        }\n",
    "        \n",
    "        results.append(analysis)\n",
    "        \n",
    "        # Create visualization for first few parameter sets\n",
    "        if i < 3:\n",
    "            fig = plt.figure(figsize=(12, 4))\n",
    "            \n",
    "            # Time series\n",
    "            plt.subplot(1, 3, 1)\n",
    "            plt.plot(time_points, x, label='X')\n",
    "            plt.plot(time_points, y, label='Y')\n",
    "            plt.plot(time_points, z, label='Z')\n",
    "            plt.xlabel('Time')\n",
    "            plt.ylabel('State')\n",
    "            plt.title(f'Lorenz System (œÉ={params[\"sigma\"]}, œÅ={params[\"rho\"]}, Œ≤={params[\"beta\"]})')\n",
    "            plt.legend()\n",
    "            \n",
    "            # Phase space (X-Y)\n",
    "            plt.subplot(1, 3, 2)\n",
    "            plt.plot(x, y)\n",
    "            plt.xlabel('X')\n",
    "            plt.ylabel('Y')\n",
    "            plt.title('X-Y Phase Space')\n",
    "            \n",
    "            # Phase space (X-Z)\n",
    "            plt.subplot(1, 3, 3)\n",
    "            plt.plot(x, z)\n",
    "            plt.xlabel('X')\n",
    "            plt.ylabel('Z')\n",
    "            plt.title('X-Z Phase Space')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Save plot to Cloud Storage\n",
    "            if storage_config:\n",
    "                img_buffer = io.BytesIO()\n",
    "                plt.savefig(img_buffer, format='png', dpi=150, bbox_inches='tight')\n",
    "                img_buffer.seek(0)\n",
    "                \n",
    "                storage_client = storage.Client(project=storage_config['project_id'])\n",
    "                bucket = storage_client.bucket(storage_config['bucket_name'])\n",
    "                \n",
    "                plot_blob = bucket.blob(f\"plots/lorenz_simulation_{i}.png\")\n",
    "                plot_blob.upload_from_string(img_buffer.getvalue(), content_type='image/png')\n",
    "            \n",
    "            plt.close()\n",
    "    \n",
    "    computation_time = time.time() - start_time\n",
    "    \n",
    "    # Save detailed results to Cloud Storage\n",
    "    if storage_config:\n",
    "        storage_client = storage.Client(project=storage_config['project_id'])\n",
    "        bucket = storage_client.bucket(storage_config['bucket_name'])\n",
    "        \n",
    "        results_blob = bucket.blob(\"results/simulation_results.pkl\")\n",
    "        results_bytes = pickle.dumps({\n",
    "            'simulation_params': simulation_params,\n",
    "            'results': results,\n",
    "            'computation_time': computation_time,\n",
    "            'timestamp': time.time()\n",
    "        })\n",
    "        results_blob.upload_from_string(results_bytes)\n",
    "    \n",
    "    return {\n",
    "        'num_simulations': len(parameter_sets),\n",
    "        'computation_time': computation_time,\n",
    "        'average_energy': np.mean([r['mean_energy'] for r in results]),\n",
    "        'max_energy': max([r['mean_energy'] for r in results]),\n",
    "        'min_energy': min([r['mean_energy'] for r in results]),\n",
    "        'results_summary': results[:3],  # First 3 for brevity\n",
    "        'storage_location': f\"gs://{storage_config['bucket_name']}/results/\" if storage_config else None\n",
    "    }\n",
    "\n",
    "# Example usage:\n",
    "# simulation_config = {\n",
    "#     'parameter_sets': [\n",
    "#         {'sigma': 10.0, 'rho': 28.0, 'beta': 8.0/3.0},\n",
    "#         {'sigma': 10.0, 'rho': 24.0, 'beta': 8.0/3.0},\n",
    "#         {'sigma': 10.0, 'rho': 32.0, 'beta': 8.0/3.0},\n",
    "#     ],\n",
    "#     'max_time': 25.0,\n",
    "#     'num_points': 10000\n",
    "# }\n",
    "# \n",
    "# storage_config = {\n",
    "#     'project_id': 'your-project-id',\n",
    "#     'bucket_name': 'your-results-bucket'\n",
    "# }\n",
    "# \n",
    "# result = gcp_scientific_simulation(simulation_config, storage_config)\n",
    "# print(f\"Completed {result['num_simulations']} simulations in {result['computation_time']:.2f} seconds\")\n",
    "# print(f\"Average energy: {result['average_energy']:.4f}\")\n",
    "\n",
    "print(\"Advanced scientific computing example defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gcp-summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This tutorial covered:\n",
    "\n",
    "1. **Setup**: GCP authentication and Clustrix installation\n",
    "2. **Compute Engine**: Direct VM configuration and management\n",
    "3. **GKE Integration**: Kubernetes clusters for containerized workloads\n",
    "4. **Cloud Batch**: Managed job scheduling for large-scale processing\n",
    "5. **Cloud Storage**: Data management and result storage\n",
    "6. **Vertex AI**: Machine learning platform integration\n",
    "7. **Cost Optimization**: Preemptible VMs and cost management strategies\n",
    "8. **Security**: Best practices for secure deployment\n",
    "9. **Resource Management**: Proper cleanup procedures\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Set up your GCP credentials and test the basic configuration\n",
    "- Start with a simple Compute Engine instance for initial testing\n",
    "- Consider GKE for containerized workloads and auto-scaling\n",
    "- Explore Cloud Batch for large-scale batch processing\n",
    "- Implement proper monitoring and cost controls\n",
    "- Use preemptible VMs for cost-effective fault-tolerant workloads\n",
    "\n",
    "### GCP-Specific Advantages\n",
    "\n",
    "- **Preemptible/Spot VMs**: Exceptional cost savings (up to 80%)\n",
    "- **Google Kubernetes Engine**: Industry-leading managed Kubernetes\n",
    "- **Vertex AI**: Comprehensive ML platform with AutoML capabilities\n",
    "- **Global Network**: Superior network performance and global reach\n",
    "- **BigQuery Integration**: Seamless data analytics integration\n",
    "- **Sustained Use Discounts**: Automatic discounts for sustained usage\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [Google Cloud Compute Engine Documentation](https://cloud.google.com/compute/docs)\n",
    "- [Google Kubernetes Engine Documentation](https://cloud.google.com/kubernetes-engine/docs)\n",
    "- [Google Cloud Batch Documentation](https://cloud.google.com/batch/docs)\n",
    "- [Vertex AI Documentation](https://cloud.google.com/vertex-ai/docs)\n",
    "- [Google Cloud Storage Documentation](https://cloud.google.com/storage/docs)\n",
    "- [Clustrix Documentation](https://clustrix.readthedocs.io/)\n",
    "\n",
    "**Remember**: Always monitor your GCP costs and clean up resources when not in use. Use budget alerts and billing export to track spending!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}