{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "gcp-title",
   "metadata": {},
   "source": [
    "# Google Cloud Platform (GCP) Tutorial\n",
    "\n",
    "This tutorial demonstrates how to use Clustrix with Google Cloud Platform (GCP) infrastructure for scalable distributed computing.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ContextLab/clustrix/blob/master/docs/notebooks/gcp_cloud_tutorial.ipynb)\n",
    "\n",
    "## Overview\n",
    "\n",
    "GCP provides several services that integrate well with Clustrix:\n",
    "\n",
    "- **Compute Engine**: Virtual machines for compute clusters\n",
    "- **Google Kubernetes Engine (GKE)**: Managed Kubernetes clusters\n",
    "- **Batch**: Managed job scheduling service\n",
    "- **Cloud Run**: Serverless container platform\n",
    "- **Vertex AI**: Machine learning platform\n",
    "- **Cloud Storage**: Object storage for data and results\n",
    "- **VPC**: Network isolation and security\n",
    "- **Preemptible VMs**: Cost-effective compute instances\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. Google Cloud account with billing enabled\n",
    "2. Google Cloud SDK (gcloud) installed and configured\n",
    "3. SSH key pair for VM access\n",
    "4. Basic understanding of GCP services"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "installation",
   "metadata": {},
   "source": [
    "## Installation and Setup\n",
    "\n",
    "Install Clustrix with GCP dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Clustrix with GCP support\n",
    "!pip install clustrix google-cloud-compute google-cloud-storage google-auth google-auth-oauthlib\n",
    "\n",
    "# Import required libraries\n",
    "import clustrix\n",
    "from clustrix import cluster, configure\n",
    "from google.cloud import compute_v1\n",
    "from google.cloud import storage\n",
    "from google.auth import default\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gcp-authentication",
   "metadata": {},
   "source": [
    "## GCP Authentication Setup\n",
    "\n",
    "Configure your GCP credentials. You can do this in several ways:\n",
    "\n",
    "### Option 1: gcloud CLI Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gcloud-auth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login with gcloud CLI (run this in terminal)\n",
    "# gcloud auth login\n",
    "# gcloud auth application-default login\n",
    "\n",
    "# Set your default project\n",
    "# gcloud config set project YOUR_PROJECT_ID\n",
    "\n",
    "# Verify authentication\n",
    "!gcloud auth list\n",
    "!gcloud config get-value project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gcp-service-account",
   "metadata": {},
   "source": [
    "### Option 2: Service Account Authentication"
   ]
  },
  {
   "cell_type": "code",
   "id": "service-account",
   "metadata": {},
   "outputs": [],
   "source": "# Set service account key path\n# os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/path/to/service-account-key.json'\n\n# Test GCP connection\ntry:\n    credentials, project_id = default()\n    print(f\"Successfully authenticated with project: {project_id}\")\n    \n    # Test compute API\n    compute_client = compute_v1.InstancesClient()\n    print(\"Compute Engine API access confirmed\")\n    \n    # Test storage API\n    storage_client = storage.Client()\n    print(\"Cloud Storage API access confirmed\")\n    \nexcept Exception as e:\n    print(f\"GCP authentication failed: {e}\")"
  },
  {
   "cell_type": "markdown",
   "id": "eegtd4c0im",
   "source": "**Make sure you have set up authentication and enabled required APIs.**",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "compute-engine-setup",
   "metadata": {},
   "source": [
    "## Method 1: Google Compute Engine Configuration\n",
    "\n",
    "### Create Compute Engine Instance for Clustrix"
   ]
  },
  {
   "cell_type": "code",
   "id": "compute-engine-creation",
   "metadata": {},
   "outputs": [],
   "source": "def create_clustrix_compute_instance(project_id, zone='us-central1-a', machine_type='e2-standard-4'):\n    \"\"\"\n    Create a GCP Compute Engine instance configured for Clustrix.\n    \n    Args:\n        project_id: GCP project ID\n        zone: GCP zone for the instance\n        machine_type: Machine type (CPU/memory configuration)\n    \n    Returns:\n        Instance configuration and gcloud commands\n    \"\"\"\n    \n    # Startup script for instance initialization\n    startup_script = '''\n#!/bin/bash\n\n# Update system\napt-get update\napt-get install -y python3 python3-pip git htop curl\n\n# Install clustrix and common packages\npip3 install clustrix numpy scipy pandas scikit-learn matplotlib\n\n# Install uv for faster package management\ncurl -LsSf https://astral.sh/uv/install.sh | sh\nsource ~/.cargo/env\n\n# Create clustrix user\nuseradd -m -s /bin/bash clustrix\nusermod -aG sudo clustrix\necho \"clustrix ALL=(ALL) NOPASSWD:ALL\" >> /etc/sudoers\n\n# Setup SSH for clustrix user\nmkdir -p /home/clustrix/.ssh\n# Copy SSH keys from default user\nif [ -d \"/home/$(logname)/.ssh\" ]; then\n    cp -r /home/$(logname)/.ssh/* /home/clustrix/.ssh/\n    chown -R clustrix:clustrix /home/clustrix/.ssh\n    chmod 700 /home/clustrix/.ssh\n    chmod 600 /home/clustrix/.ssh/authorized_keys 2>/dev/null || true\nfi\n\n# Create working directory\nmkdir -p /tmp/clustrix\nchown clustrix:clustrix /tmp/clustrix\n\n# Install Google Cloud SDK for clustrix user\ncurl https://sdk.cloud.google.com | bash\nexec -l $SHELL\n\n# Log completion\necho \"Clustrix setup completed at $(date)\" >> /var/log/clustrix-setup.log\n'''\n    \n    # gcloud commands for instance creation\n    gcloud_commands = f\"\"\"\n# Create firewall rule for SSH (if not exists)\ngcloud compute firewall-rules create allow-ssh \\\n  --allow tcp:22 \\\n  --source-ranges 0.0.0.0/0 \\\n  --description \"Allow SSH access\" \\\n  --project {project_id} || echo \"SSH rule already exists\"\n\n# Create the instance\ngcloud compute instances create clustrix-instance \\\n  --project={project_id} \\\n  --zone={zone} \\\n  --machine-type={machine_type} \\\n  --network-interface=network-tier=PREMIUM,subnet=default \\\n  --maintenance-policy=MIGRATE \\\n  --provisioning-model=STANDARD \\\n  --service-account=default \\\n  --scopes=https://www.googleapis.com/auth/cloud-platform \\\n  --tags=clustrix,http-server,https-server \\\n  --create-disk=auto-delete=yes,boot=yes,device-name=clustrix-instance,image=projects/ubuntu-os-cloud/global/images/family/ubuntu-2204-lts,mode=rw,size=50,type=projects/{project_id}/zones/{zone}/diskTypes/pd-balanced \\\n  --no-shielded-secure-boot \\\n  --shielded-vtpm \\\n  --shielded-integrity-monitoring \\\n  --labels=purpose=clustrix,environment=tutorial \\\n  --reservation-affinity=any \\\n  --metadata-from-file startup-script=startup-script.sh\n\n# Get the external IP\ngcloud compute instances describe clustrix-instance \\\n  --project={project_id} \\\n  --zone={zone} \\\n  --format='get(networkInterfaces[0].accessConfigs[0].natIP)'\n\n# SSH to the instance (after startup script completes)\ngcloud compute ssh clustrix-instance \\\n  --project={project_id} \\\n  --zone={zone}\n\"\"\"\n    \n    return {\n        'project_id': project_id,\n        'zone': zone,\n        'machine_type': machine_type,\n        'instance_name': 'clustrix-instance',\n        'gcloud_commands': gcloud_commands,\n        'startup_script': startup_script\n    }\n\n# Example usage\ninstance_config = create_clustrix_compute_instance(\n    project_id='your-project-id',  # Replace with your project ID\n    zone='us-central1-a',\n    machine_type='e2-standard-4'  # 4 vCPUs, 16 GB RAM\n)\n\nprint(\"GCP Compute Engine Instance Creation:\")\nprint(instance_config['gcloud_commands'])\nprint(\"\\nStartup Script:\")\nprint(instance_config['startup_script'])"
  },
  {
   "cell_type": "markdown",
   "id": "nrfay0eolc",
   "source": "**Save the startup script and execute the gcloud commands to create your instance.**",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "clustrix-gcp-config",
   "metadata": {},
   "source": [
    "### Configure Clustrix for Compute Engine"
   ]
  },
  {
   "cell_type": "code",
   "id": "config-gcp-compute",
   "metadata": {},
   "outputs": [],
   "source": "# Configure Clustrix to use your Compute Engine instance\nconfigure(\n    cluster_type=\"ssh\",\n    cluster_host=\"your-instance-external-ip\",  # Replace with actual IP\n    username=\"clustrix\",  # or your default user\n    key_file=\"~/.ssh/google_compute_engine\",  # gcloud-generated key\n    remote_work_dir=\"/tmp/clustrix\",\n    package_manager=\"auto\",  # Will use uv if available\n    default_cores=4,\n    default_memory=\"8GB\",\n    default_time=\"01:00:00\"\n)"
  },
  {
   "cell_type": "markdown",
   "id": "6qtk506dlio",
   "source": "**Replace `your-instance-external-ip` with the actual external IP from your Compute Engine instance.**",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "gcp-example",
   "metadata": {},
   "source": [
    "### Example: Remote Computation on Compute Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gcp-compute-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cluster(cores=2, memory=\"4GB\")\n",
    "def gcp_data_analysis(dataset_size=10000, analysis_type='regression'):\n",
    "    \"\"\"Perform data analysis on GCP Compute Engine.\"\"\"\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "    from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "    from sklearn.datasets import make_regression, make_classification\n",
    "    import time\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Generate synthetic dataset\n",
    "    if analysis_type == 'regression':\n",
    "        X, y = make_regression(\n",
    "            n_samples=dataset_size,\n",
    "            n_features=20,\n",
    "            noise=0.1,\n",
    "            random_state=42\n",
    "        )\n",
    "        model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "        metric_name = 'rmse'\n",
    "    else:\n",
    "        X, y = make_classification(\n",
    "            n_samples=dataset_size,\n",
    "            n_features=20,\n",
    "            n_classes=3,\n",
    "            random_state=42\n",
    "        )\n",
    "        model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "        metric_name = 'accuracy'\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    training_start = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    training_time = time.time() - training_start\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    if analysis_type == 'regression':\n",
    "        metric_value = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    else:\n",
    "        metric_value = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    return {\n",
    "        'analysis_type': analysis_type,\n",
    "        'dataset_size': dataset_size,\n",
    "        'training_time': training_time,\n",
    "        'total_time': total_time,\n",
    "        metric_name: metric_value,\n",
    "        'feature_importance': model.feature_importances_[:5].tolist(),  # Top 5\n",
    "        'training_samples': len(X_train),\n",
    "        'test_samples': len(X_test)\n",
    "    }\n",
    "\n",
    "# Run computation on GCP Compute Engine\n",
    "# result = gcp_data_analysis(dataset_size=50000, analysis_type='classification')\n",
    "# print(f\"Analysis completed: {result['accuracy']:.4f} accuracy in {result['total_time']:.2f} seconds\")\n",
    "print(\"Example function defined. Uncomment the lines above to run on your GCP instance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gke-setup",
   "metadata": {},
   "source": [
    "## Method 2: Google Kubernetes Engine (GKE) Configuration\n",
    "\n",
    "GKE provides managed Kubernetes clusters ideal for containerized Clustrix workloads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gke-cluster-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_gke_cluster_for_clustrix(project_id, cluster_name='clustrix-cluster', zone='us-central1-a'):\n",
    "    \"\"\"\n",
    "    Setup GKE cluster optimized for Clustrix workloads.\n",
    "    \"\"\"\n",
    "    \n",
    "    gke_commands = f\"\"\"\n",
    "# Enable required APIs\n",
    "gcloud services enable container.googleapis.com \\\n",
    "  --project {project_id}\n",
    "\n",
    "# Create GKE cluster with auto-scaling\n",
    "gcloud container clusters create {cluster_name} \\\n",
    "  --project {project_id} \\\n",
    "  --zone {zone} \\\n",
    "  --machine-type e2-standard-4 \\\n",
    "  --num-nodes 1 \\\n",
    "  --enable-autoscaling \\\n",
    "  --min-nodes 0 \\\n",
    "  --max-nodes 10 \\\n",
    "  --enable-autorepair \\\n",
    "  --enable-autoupgrade \\\n",
    "  --disk-size 50GB \\\n",
    "  --disk-type pd-ssd \\\n",
    "  --enable-network-policy \\\n",
    "  --enable-ip-alias \\\n",
    "  --labels purpose=clustrix,environment=tutorial\n",
    "\n",
    "# Get cluster credentials\n",
    "gcloud container clusters get-credentials {cluster_name} \\\n",
    "  --project {project_id} \\\n",
    "  --zone {zone}\n",
    "\n",
    "# Verify cluster access\n",
    "kubectl get nodes\n",
    "\n",
    "# Create clustrix namespace\n",
    "kubectl create namespace clustrix\n",
    "\n",
    "# Set as default namespace\n",
    "kubectl config set-context --current --namespace=clustrix\n",
    "\"\"\"\n",
    "    \n",
    "    # Clustrix job template for Kubernetes\n",
    "    k8s_job_template = \"\"\"\n",
    "apiVersion: batch/v1\n",
    "kind: Job\n",
    "metadata:\n",
    "  name: clustrix-job-${JOB_ID}\n",
    "  namespace: clustrix\n",
    "spec:\n",
    "  template:\n",
    "    spec:\n",
    "      restartPolicy: Never\n",
    "      containers:\n",
    "      - name: clustrix-worker\n",
    "        image: python:3.11-slim\n",
    "        command: [\"bash\", \"-c\"]\n",
    "        args:\n",
    "        - |\n",
    "          pip install clustrix numpy scipy pandas scikit-learn\n",
    "          python -c \"\n",
    "          import pickle\n",
    "          import sys\n",
    "          \n",
    "          # Load and execute function\n",
    "          with open('/data/function_data.pkl', 'rb') as f:\n",
    "              data = pickle.load(f)\n",
    "          \n",
    "          func = pickle.loads(data['function'])\n",
    "          args = pickle.loads(data['args'])\n",
    "          kwargs = pickle.loads(data['kwargs'])\n",
    "          \n",
    "          try:\n",
    "              result = func(*args, **kwargs)\n",
    "              with open('/data/result.pkl', 'wb') as f:\n",
    "                  pickle.dump(result, f)\n",
    "          except Exception as e:\n",
    "              with open('/data/error.pkl', 'wb') as f:\n",
    "                  pickle.dump({'error': str(e)}, f)\n",
    "              raise\n",
    "          \"\n",
    "        resources:\n",
    "          requests:\n",
    "            memory: \"2Gi\"\n",
    "            cpu: \"1\"\n",
    "          limits:\n",
    "            memory: \"4Gi\"\n",
    "            cpu: \"2\"\n",
    "        volumeMounts:\n",
    "        - name: job-data\n",
    "          mountPath: /data\n",
    "      volumes:\n",
    "      - name: job-data\n",
    "        persistentVolumeClaim:\n",
    "          claimName: clustrix-pvc\n",
    "  backoffLimit: 3\n",
    "\"\"\"\n",
    "    \n",
    "    print(\"GKE Cluster Setup Commands:\")\n",
    "    print(gke_commands)\n",
    "    print(\"\\nKubernetes Job Template:\")\n",
    "    print(k8s_job_template)\n",
    "    \n",
    "    return {\n",
    "        'cluster_name': cluster_name,\n",
    "        'project_id': project_id,\n",
    "        'zone': zone,\n",
    "        'setup_commands': gke_commands,\n",
    "        'job_template': k8s_job_template\n",
    "    }\n",
    "\n",
    "def configure_clustrix_for_gke(cluster_endpoint, cluster_name):\n",
    "    \"\"\"Configure Clustrix to use GKE cluster.\"\"\"\n",
    "    configure(\n",
    "        cluster_type=\"kubernetes\",\n",
    "        cluster_host=cluster_endpoint,\n",
    "        # For GKE, authentication is handled via kubectl config\n",
    "        remote_work_dir=\"/tmp/clustrix\",\n",
    "        package_manager=\"pip\",  # Container-based, pip is fine\n",
    "        default_cores=2,\n",
    "        default_memory=\"4GB\",\n",
    "        default_time=\"01:00:00\"\n",
    "    )\n",
    "    print(f\"Configured Clustrix for GKE cluster: {cluster_name}\")\n",
    "\n",
    "gke_config = setup_gke_cluster_for_clustrix(\n",
    "    project_id='your-project-id',\n",
    "    cluster_name='clustrix-cluster'\n",
    ")\n",
    "\n",
    "print(\"\\nNote: GKE integration requires additional implementation in Clustrix.\")\n",
    "print(\"Current Clustrix supports basic Kubernetes, but GKE-specific features need custom setup.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gcp-batch",
   "metadata": {},
   "source": [
    "## Method 3: Google Cloud Batch\n",
    "\n",
    "Google Cloud Batch provides managed job scheduling for large-scale workloads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gcp-batch-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_gcp_batch_environment(project_id, region='us-central1'):\n",
    "    \"\"\"\n",
    "    Setup Google Cloud Batch for Clustrix workloads.\n",
    "    \"\"\"\n",
    "    \n",
    "    batch_setup_commands = f\"\"\"\n",
    "# Enable Batch API\n",
    "gcloud services enable batch.googleapis.com \\\n",
    "  --project {project_id}\n",
    "\n",
    "# Create a service account for Batch jobs\n",
    "gcloud iam service-accounts create clustrix-batch-sa \\\n",
    "  --project {project_id} \\\n",
    "  --description=\"Service account for Clustrix Batch jobs\" \\\n",
    "  --display-name=\"Clustrix Batch Service Account\"\n",
    "\n",
    "# Grant necessary permissions\n",
    "gcloud projects add-iam-policy-binding {project_id} \\\n",
    "  --member=\"serviceAccount:clustrix-batch-sa@{project_id}.iam.gserviceaccount.com\" \\\n",
    "  --role=\"roles/batch.jobsEditor\"\n",
    "\n",
    "gcloud projects add-iam-policy-binding {project_id} \\\n",
    "  --member=\"serviceAccount:clustrix-batch-sa@{project_id}.iam.gserviceaccount.com\" \\\n",
    "  --role=\"roles/storage.objectAdmin\"\n",
    "\n",
    "# Create Cloud Storage bucket for job data\n",
    "gsutil mb -p {project_id} -l {region} gs://{project_id}-clustrix-batch\n",
    "\"\"\"\n",
    "    \n",
    "    # Batch job configuration template\n",
    "    batch_job_config = {\n",
    "        \"taskGroups\": [\n",
    "            {\n",
    "                \"taskSpec\": {\n",
    "                    \"runnables\": [\n",
    "                        {\n",
    "                            \"script\": {\n",
    "                                \"text\": \"\"\"\n",
    "#!/bin/bash\n",
    "set -e\n",
    "\n",
    "# Install required packages\n",
    "pip3 install clustrix numpy scipy pandas scikit-learn\n",
    "\n",
    "# Download job data from Cloud Storage\n",
    "gsutil cp gs://{project_id}-clustrix-batch/jobs/${{BATCH_JOB_ID}}/function_data.pkl .\n",
    "\n",
    "# Execute the function\n",
    "python3 -c \"\n",
    "import pickle\n",
    "import traceback\n",
    "\n",
    "try:\n",
    "    with open('function_data.pkl', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    func = pickle.loads(data['function'])\n",
    "    args = pickle.loads(data['args'])\n",
    "    kwargs = pickle.loads(data['kwargs'])\n",
    "    \n",
    "    result = func(*args, **kwargs)\n",
    "    \n",
    "    with open('result.pkl', 'wb') as f:\n",
    "        pickle.dump(result, f)\n",
    "        \n",
    "except Exception as e:\n",
    "    with open('error.pkl', 'wb') as f:\n",
    "        pickle.dump({{\n",
    "            'error': str(e),\n",
    "            'traceback': traceback.format_exc()\n",
    "        }}, f)\n",
    "    raise\n",
    "\"\n",
    "\n",
    "# Upload results to Cloud Storage\n",
    "gsutil cp result.pkl gs://{project_id}-clustrix-batch/jobs/${{BATCH_JOB_ID}}/result.pkl || \\\n",
    "gsutil cp error.pkl gs://{project_id}-clustrix-batch/jobs/${{BATCH_JOB_ID}}/error.pkl\n",
    "\"\"\"\n",
    "                            }\n",
    "                        }\n",
    "                    ],\n",
    "                    \"computeResource\": {\n",
    "                        \"cpuMilli\": 2000,  # 2 CPUs\n",
    "                        \"memoryMib\": 4096  # 4 GB RAM\n",
    "                    },\n",
    "                    \"maxRetryCount\": 2,\n",
    "                    \"maxRunDuration\": \"3600s\"  # 1 hour\n",
    "                },\n",
    "                \"taskCount\": 1\n",
    "            }\n",
    "        ],\n",
    "        \"allocationPolicy\": {\n",
    "            \"instances\": [\n",
    "                {\n",
    "                    \"instanceTemplate\": {\n",
    "                        \"machineType\": \"e2-standard-2\",\n",
    "                        \"provisioningModel\": \"STANDARD\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        \"labels\": {\n",
    "            \"purpose\": \"clustrix\",\n",
    "            \"environment\": \"tutorial\"\n",
    "        },\n",
    "        \"logsPolicy\": {\n",
    "            \"destination\": \"CLOUD_LOGGING\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"Google Cloud Batch Setup Commands:\")\n",
    "    print(batch_setup_commands)\n",
    "    print(\"\\nBatch Job Configuration:\")\n",
    "    print(json.dumps(batch_job_config, indent=2))\n",
    "    \n",
    "    return {\n",
    "        'project_id': project_id,\n",
    "        'region': region,\n",
    "        'bucket_name': f'{project_id}-clustrix-batch',\n",
    "        'service_account': f'clustrix-batch-sa@{project_id}.iam.gserviceaccount.com',\n",
    "        'job_config': batch_job_config,\n",
    "        'setup_commands': batch_setup_commands\n",
    "    }\n",
    "\n",
    "batch_config = setup_gcp_batch_environment('your-project-id')\n",
    "print(\"\\nGoogle Cloud Batch provides excellent integration for large-scale Clustrix workloads.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cloud-storage",
   "metadata": {},
   "source": [
    "## Data Management with Google Cloud Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cloud-storage-integration",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cluster(cores=2, memory=\"4GB\")\n",
    "def process_gcs_data(bucket_name, input_blob, output_blob, project_id=None):\n",
    "    \"\"\"Process data from Google Cloud Storage and save results back.\"\"\"\n",
    "    from google.cloud import storage\n",
    "    import numpy as np\n",
    "    import pickle\n",
    "    import io\n",
    "    import time\n",
    "    \n",
    "    # Initialize Cloud Storage client\n",
    "    storage_client = storage.Client(project=project_id)\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    \n",
    "    # Download data from Cloud Storage\n",
    "    input_blob_obj = bucket.blob(input_blob)\n",
    "    data_bytes = input_blob_obj.download_as_bytes()\n",
    "    data = pickle.loads(data_bytes)\n",
    "    \n",
    "    # Process the data\n",
    "    processed_data = {\n",
    "        'original_shape': data.shape if hasattr(data, 'shape') else len(data) if hasattr(data, '__len__') else 'scalar',\n",
    "        'mean': float(np.mean(data)) if hasattr(data, '__iter__') else float(data),\n",
    "        'std': float(np.std(data)) if hasattr(data, '__iter__') else 0.0,\n",
    "        'max': float(np.max(data)) if hasattr(data, '__iter__') else float(data),\n",
    "        'min': float(np.min(data)) if hasattr(data, '__iter__') else float(data),\n",
    "        'processing_timestamp': time.time(),\n",
    "        'processed_on': 'gcp-compute-engine',\n",
    "        'data_type': str(type(data).__name__)\n",
    "    }\n",
    "    \n",
    "    # Advanced processing based on data type\n",
    "    if hasattr(data, 'shape') and len(data.shape) >= 2:\n",
    "        # Matrix operations\n",
    "        processed_data.update({\n",
    "            'matrix_rank': int(np.linalg.matrix_rank(data)) if data.shape[0] == data.shape[1] else 'non_square',\n",
    "            'frobenius_norm': float(np.linalg.norm(data, 'fro')),\n",
    "            'condition_number': float(np.linalg.cond(data)) if data.shape[0] == data.shape[1] else None\n",
    "        })\n",
    "    \n",
    "    # Upload results to Cloud Storage\n",
    "    output_bytes = pickle.dumps(processed_data)\n",
    "    output_blob_obj = bucket.blob(output_blob)\n",
    "    output_blob_obj.upload_from_string(output_bytes)\n",
    "    \n",
    "    return f\"Processed data saved to gs://{bucket_name}/{output_blob}\"\n",
    "\n",
    "# Utility functions for Google Cloud Storage\n",
    "def upload_to_gcs(data, bucket_name, blob_name, project_id=None):\n",
    "    \"\"\"Upload data to Google Cloud Storage.\"\"\"\n",
    "    storage_client = storage.Client(project=project_id)\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "    \n",
    "    data_bytes = pickle.dumps(data)\n",
    "    blob.upload_from_string(data_bytes)\n",
    "    print(f\"Data uploaded to gs://{bucket_name}/{blob_name}\")\n",
    "\n",
    "def download_from_gcs(bucket_name, blob_name, project_id=None):\n",
    "    \"\"\"Download data from Google Cloud Storage.\"\"\"\n",
    "    storage_client = storage.Client(project=project_id)\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "    \n",
    "    data_bytes = blob.download_as_bytes()\n",
    "    return pickle.loads(data_bytes)\n",
    "\n",
    "def create_gcs_bucket_for_clustrix(project_id, bucket_name, location='us-central1'):\n",
    "    \"\"\"Create a Cloud Storage bucket for Clustrix data.\"\"\"\n",
    "    gcs_commands = f\"\"\"\n",
    "# Create bucket with appropriate settings\n",
    "gsutil mb -p {project_id} -l {location} gs://{bucket_name}\n",
    "\n",
    "# Set lifecycle policy to delete temporary files after 7 days\n",
    "echo '{{\n",
    "  \"lifecycle\": {{\n",
    "    \"rule\": [\n",
    "      {{\n",
    "        \"action\": {{\"type\": \"Delete\"}},\n",
    "        \"condition\": {{\n",
    "          \"age\": 7,\n",
    "          \"matchesPrefix\": [\"temp/\"]\n",
    "        }}\n",
    "      }}\n",
    "    ]\n",
    "  }}\n",
    "}}' > lifecycle.json\n",
    "\n",
    "gsutil lifecycle set lifecycle.json gs://{bucket_name}\n",
    "\n",
    "# Set up proper permissions (if using service account)\n",
    "gsutil iam ch serviceAccount:clustrix-batch-sa@{project_id}.iam.gserviceaccount.com:objectAdmin gs://{bucket_name}\n",
    "\"\"\"\n",
    "    \n",
    "    print(f\"Commands to create bucket gs://{bucket_name}:\")\n",
    "    print(gcs_commands)\n",
    "    return gcs_commands\n",
    "\n",
    "# Example usage:\n",
    "# Create bucket first\n",
    "bucket_commands = create_gcs_bucket_for_clustrix('your-project-id', 'your-clustrix-data')\n",
    "\n",
    "# Then use the functions\n",
    "# sample_data = np.random.rand(1000, 100)\n",
    "# upload_to_gcs(sample_data, 'your-clustrix-data', 'input/sample_data.pkl', 'your-project-id')\n",
    "# result = process_gcs_data('your-clustrix-data', 'input/sample_data.pkl', 'output/results.pkl', 'your-project-id')\n",
    "print(\"\\nGoogle Cloud Storage integration functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vertex-ai",
   "metadata": {},
   "source": [
    "## Vertex AI Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vertex-ai-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_vertex_ai_for_clustrix(project_id, region='us-central1'):\n",
    "    \"\"\"\n",
    "    Setup Vertex AI for ML workloads with Clustrix.\n",
    "    \"\"\"\n",
    "    \n",
    "    vertex_commands = f\"\"\"\n",
    "# Enable Vertex AI API\n",
    "gcloud services enable aiplatform.googleapis.com \\\n",
    "  --project {project_id}\n",
    "\n",
    "# Create Vertex AI custom training job\n",
    "gcloud ai custom-jobs create \\\n",
    "  --region={region} \\\n",
    "  --display-name=clustrix-training-job \\\n",
    "  --config=training_job_config.yaml\n",
    "\n",
    "# Create Vertex AI endpoints for model serving\n",
    "gcloud ai endpoints create \\\n",
    "  --region={region} \\\n",
    "  --display-name=clustrix-model-endpoint\n",
    "\"\"\"\n",
    "    \n",
    "    # Vertex AI training job configuration\n",
    "    training_config = \"\"\"\n",
    "# training_job_config.yaml\n",
    "workerPoolSpecs:\n",
    "- machineSpec:\n",
    "    machineType: e2-standard-4\n",
    "  replicaCount: 1\n",
    "  containerSpec:\n",
    "    imageUri: gcr.io/cloud-aiplatform/training/tf-cpu.2-8:latest\n",
    "    command:\n",
    "    - python3\n",
    "    - -c\n",
    "    args:\n",
    "    - |\n",
    "      import subprocess\n",
    "      import sys\n",
    "      \n",
    "      # Install clustrix\n",
    "      subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'clustrix', 'numpy', 'pandas', 'scikit-learn'])\n",
    "      \n",
    "      # Your training code here\n",
    "      print(\"Clustrix training job completed on Vertex AI\")\n",
    "    env:\n",
    "    - name: GOOGLE_CLOUD_PROJECT\n",
    "      value: {project_id}\n",
    "    - name: AIP_MODEL_DIR\n",
    "      value: gs://{project_id}-vertex-models\n",
    "\"\"\"\n",
    "    \n",
    "    print(\"Vertex AI Setup Commands:\")\n",
    "    print(vertex_commands)\n",
    "    print(\"\\nTraining Job Configuration:\")\n",
    "    print(training_config)\n",
    "    \n",
    "    return {\n",
    "        'project_id': project_id,\n",
    "        'region': region,\n",
    "        'setup_commands': vertex_commands,\n",
    "        'training_config': training_config\n",
    "    }\n",
    "\n",
    "@cluster(cores=4, memory=\"8GB\")\n",
    "def vertex_ai_ml_pipeline(dataset_config, model_config, project_id, bucket_name):\n",
    "    \"\"\"ML pipeline that could run on Vertex AI with Clustrix.\"\"\"\n",
    "    import numpy as np\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "    from sklearn.datasets import make_classification\n",
    "    from sklearn.metrics import classification_report\n",
    "    from google.cloud import storage\n",
    "    import pickle\n",
    "    import time\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Generate or load dataset\n",
    "    X, y = make_classification(\n",
    "        n_samples=dataset_config['n_samples'],\n",
    "        n_features=dataset_config['n_features'],\n",
    "        n_classes=dataset_config['n_classes'],\n",
    "        n_informative=dataset_config.get('n_informative', dataset_config['n_features'] // 2),\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Hyperparameter tuning\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.1, 0.2]\n",
    "    }\n",
    "    \n",
    "    # Grid search with cross-validation\n",
    "    model = GradientBoostingClassifier(random_state=42)\n",
    "    grid_search = GridSearchCV(\n",
    "        model, param_grid, cv=5, scoring='accuracy', n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X, y)\n",
    "    \n",
    "    # Get best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    # Evaluate with cross-validation\n",
    "    cv_scores = cross_val_score(best_model, X, y, cv=5, scoring='accuracy')\n",
    "    \n",
    "    # Save model to Cloud Storage\n",
    "    storage_client = storage.Client(project=project_id)\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    \n",
    "    model_blob = bucket.blob('models/clustrix_model.pkl')\n",
    "    model_bytes = pickle.dumps(best_model)\n",
    "    model_blob.upload_from_string(model_bytes)\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    return {\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'best_score': grid_search.best_score_,\n",
    "        'cv_mean_score': cv_scores.mean(),\n",
    "        'cv_std_score': cv_scores.std(),\n",
    "        'training_time': total_time,\n",
    "        'model_location': f'gs://{bucket_name}/models/clustrix_model.pkl',\n",
    "        'feature_importance': best_model.feature_importances_[:10].tolist(),  # Top 10\n",
    "        'dataset_size': len(X)\n",
    "    }\n",
    "\n",
    "vertex_config = setup_vertex_ai_for_clustrix('your-project-id')\n",
    "\n",
    "# Example usage:\n",
    "# dataset_params = {'n_samples': 10000, 'n_features': 20, 'n_classes': 3}\n",
    "# model_params = {}\n",
    "# result = vertex_ai_ml_pipeline(dataset_params, model_params, 'your-project-id', 'your-bucket')\n",
    "# print(f\"Best model score: {result['best_score']:.4f}\")\n",
    "\n",
    "print(\"Vertex AI integration examples defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preemptible-vms",
   "metadata": {},
   "source": [
    "## Cost Optimization with Preemptible VMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preemptible-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_preemptible_cluster(project_id, zone='us-central1-a'):\n",
    "    \"\"\"\n",
    "    Setup cost-effective cluster using preemptible VMs.\n",
    "    \"\"\"\n",
    "    \n",
    "    preemptible_commands = f\"\"\"\n",
    "# Create preemptible instance template\n",
    "gcloud compute instance-templates create clustrix-preemptible-template \\\n",
    "  --project {project_id} \\\n",
    "  --machine-type e2-standard-4 \\\n",
    "  --preemptible \\\n",
    "  --boot-disk-size 50GB \\\n",
    "  --boot-disk-type pd-standard \\\n",
    "  --image-family ubuntu-2204-lts \\\n",
    "  --image-project ubuntu-os-cloud \\\n",
    "  --metadata-from-file startup-script=clustrix-startup.sh \\\n",
    "  --scopes cloud-platform \\\n",
    "  --tags clustrix,preemptible\n",
    "\n",
    "# Create managed instance group\n",
    "gcloud compute instance-groups managed create clustrix-preemptible-group \\\n",
    "  --project {project_id} \\\n",
    "  --zone {zone} \\\n",
    "  --template clustrix-preemptible-template \\\n",
    "  --size 0\n",
    "\n",
    "# Set up auto-scaling\n",
    "gcloud compute instance-groups managed set-autoscaling clustrix-preemptible-group \\\n",
    "  --project {project_id} \\\n",
    "  --zone {zone} \\\n",
    "  --min-num-replicas 0 \\\n",
    "  --max-num-replicas 10 \\\n",
    "  --target-cpu-utilization 0.6\n",
    "\n",
    "# Scale up the group\n",
    "gcloud compute instance-groups managed resize clustrix-preemptible-group \\\n",
    "  --project {project_id} \\\n",
    "  --zone {zone} \\\n",
    "  --size 2\n",
    "\"\"\"\n",
    "    \n",
    "    cost_optimization_tips = \"\"\"\n",
    "GCP Cost Optimization for Clustrix:\n",
    "\n",
    "1. Compute Optimization:\n",
    "   - Use Preemptible VMs for fault-tolerant workloads (up to 80% savings)\n",
    "   - Use Spot VMs (successor to Preemptible) for even better savings\n",
    "   - Choose appropriate machine types (E2, N2, C2 based on workload)\n",
    "   - Use sustained use discounts for long-running workloads\n",
    "   - Consider committed use discounts for predictable usage\n",
    "\n",
    "2. Storage Optimization:\n",
    "   - Use appropriate storage classes (Standard, Nearline, Coldline, Archive)\n",
    "   - Enable object lifecycle management\n",
    "   - Use regional storage for better performance/cost balance\n",
    "   - Implement data compression and deduplication\n",
    "\n",
    "3. Network Optimization:\n",
    "   - Minimize inter-region data transfer\n",
    "   - Use Cloud CDN for static content\n",
    "   - Optimize data transfer patterns\n",
    "\n",
    "4. Monitoring and Management:\n",
    "   - Set up budget alerts and quotas\n",
    "   - Use Cloud Billing reports\n",
    "   - Implement proper resource labeling\n",
    "   - Regular cost reviews and right-sizing\n",
    "\n",
    "5. Service-Specific:\n",
    "   - Use Cloud Functions for event-driven tasks\n",
    "   - Consider Cloud Run for containerized applications\n",
    "   - Use Google Cloud Batch for large batch processing\n",
    "\"\"\"\n",
    "    \n",
    "    print(\"Preemptible VM Setup Commands:\")\n",
    "    print(preemptible_commands)\n",
    "    print(\"\\nCost Optimization Tips:\")\n",
    "    print(cost_optimization_tips)\n",
    "    \n",
    "    return {\n",
    "        'project_id': project_id,\n",
    "        'zone': zone,\n",
    "        'template_name': 'clustrix-preemptible-template',\n",
    "        'group_name': 'clustrix-preemptible-group',\n",
    "        'setup_commands': preemptible_commands\n",
    "    }\n",
    "\n",
    "def configure_for_preemptible():\n",
    "    \"\"\"Configure Clustrix for preemptible VM usage.\"\"\"\n",
    "    configure(\n",
    "        cluster_type=\"ssh\",\n",
    "        cluster_host=\"preemptible-instance-ip\",\n",
    "        username=\"clustrix\",\n",
    "        key_file=\"~/.ssh/google_compute_engine\",\n",
    "        remote_work_dir=\"/tmp/clustrix\",\n",
    "        package_manager=\"uv\",\n",
    "        # Preemptible VMs can be terminated, use shorter timeouts\n",
    "        default_time=\"00:30:00\",\n",
    "        job_poll_interval=30,  # Check more frequently\n",
    "        cleanup_on_success=True,  # Clean up quickly\n",
    "        # Save work frequently\n",
    "        max_parallel_jobs=20  # Higher parallelism for fault tolerance\n",
    "    )\n",
    "    print(\"Configured Clustrix for preemptible VMs with fault-tolerant settings.\")\n",
    "\n",
    "preemptible_config = setup_preemptible_cluster('your-project-id')\n",
    "print(\"\\nPreemptible VMs can provide up to 80% cost savings for fault-tolerant workloads.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gcp-security",
   "metadata": {},
   "source": [
    "## Security Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gcp-security-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_gcp_security_for_clustrix(project_id):\n",
    "    \"\"\"\n",
    "    Security configuration for GCP + Clustrix deployment.\n",
    "    \"\"\"\n",
    "    \n",
    "    security_commands = f\"\"\"\n",
    "# Create VPC with private subnets\n",
    "gcloud compute networks create clustrix-vpc \\\n",
    "  --project {project_id} \\\n",
    "  --subnet-mode custom\n",
    "\n",
    "gcloud compute networks subnets create clustrix-subnet \\\n",
    "  --project {project_id} \\\n",
    "  --network clustrix-vpc \\\n",
    "  --range 10.1.0.0/24 \\\n",
    "  --region us-central1 \\\n",
    "  --enable-private-ip-google-access\n",
    "\n",
    "# Create firewall rules (restrictive)\n",
    "gcloud compute firewall-rules create clustrix-allow-ssh \\\n",
    "  --project {project_id} \\\n",
    "  --network clustrix-vpc \\\n",
    "  --allow tcp:22 \\\n",
    "  --source-ranges YOUR_IP/32 \\\n",
    "  --target-tags clustrix\n",
    "\n",
    "gcloud compute firewall-rules create clustrix-internal \\\n",
    "  --project {project_id} \\\n",
    "  --network clustrix-vpc \\\n",
    "  --allow tcp,udp,icmp \\\n",
    "  --source-ranges 10.1.0.0/24 \\\n",
    "  --target-tags clustrix\n",
    "\n",
    "# Create service account with minimal permissions\n",
    "gcloud iam service-accounts create clustrix-compute \\\n",
    "  --project {project_id} \\\n",
    "  --description=\"Service account for Clustrix compute instances\" \\\n",
    "  --display-name=\"Clustrix Compute Service Account\"\n",
    "\n",
    "# Grant only necessary permissions\n",
    "gcloud projects add-iam-policy-binding {project_id} \\\n",
    "  --member=\"serviceAccount:clustrix-compute@{project_id}.iam.gserviceaccount.com\" \\\n",
    "  --role=\"roles/storage.objectAdmin\"\n",
    "\n",
    "gcloud projects add-iam-policy-binding {project_id} \\\n",
    "  --member=\"serviceAccount:clustrix-compute@{project_id}.iam.gserviceaccount.com\" \\\n",
    "  --role=\"roles/logging.logWriter\"\n",
    "\n",
    "# Enable OS Login for better SSH key management\n",
    "gcloud compute project-info add-metadata \\\n",
    "  --project {project_id} \\\n",
    "  --metadata enable-oslogin=TRUE\n",
    "\n",
    "# Create Cloud KMS key for encryption\n",
    "gcloud kms keyrings create clustrix-keyring \\\n",
    "  --project {project_id} \\\n",
    "  --location global\n",
    "\n",
    "gcloud kms keys create clustrix-key \\\n",
    "  --project {project_id} \\\n",
    "  --keyring clustrix-keyring \\\n",
    "  --location global \\\n",
    "  --purpose encryption\n",
    "\"\"\"\n",
    "    \n",
    "    security_checklist = \"\"\"\n",
    "GCP Security Checklist for Clustrix:\n",
    "\n",
    "✓ Use IAM service accounts with minimal permissions\n",
    "✓ Enable OS Login for centralized SSH key management\n",
    "✓ Create custom VPC with private subnets\n",
    "✓ Restrict firewall rules to specific IP ranges\n",
    "✓ Enable private Google access for instances without external IPs\n",
    "✓ Use Cloud KMS for encryption at rest\n",
    "✓ Enable audit logging and Cloud Security Command Center\n",
    "✓ Use Binary Authorization for container security\n",
    "✓ Implement VPC Service Controls for data perimeter\n",
    "✓ Enable DDoS protection and Cloud Armor\n",
    "✓ Use Secret Manager for sensitive configuration\n",
    "✓ Enable vulnerability scanning for container images\n",
    "✓ Set up budget alerts and billing account security\n",
    "✓ Use organization policies for governance\n",
    "✓ Regular security reviews and access audits\n",
    "\"\"\"\n",
    "    \n",
    "    print(\"GCP Security Setup Commands:\")\n",
    "    print(security_commands)\n",
    "    print(\"\\nSecurity Checklist:\")\n",
    "    print(security_checklist)\n",
    "    \n",
    "    return {\n",
    "        'project_id': project_id,\n",
    "        'vpc_name': 'clustrix-vpc',\n",
    "        'subnet_name': 'clustrix-subnet',\n",
    "        'service_account': f'clustrix-compute@{project_id}.iam.gserviceaccount.com',\n",
    "        'security_commands': security_commands\n",
    "    }\n",
    "\n",
    "security_config = setup_gcp_security_for_clustrix('your-project-id')\n",
    "print(\"Security configuration templates generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup-gcp",
   "metadata": {},
   "source": [
    "## Resource Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup-gcp-resources",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_gcp_resources(project_id, zone='us-central1-a', region='us-central1'):\n",
    "    \"\"\"\n",
    "    Clean up GCP resources to avoid ongoing charges.\n",
    "    \n",
    "    Args:\n",
    "        project_id: GCP project ID\n",
    "        zone: Zone where resources were created\n",
    "        region: Region where resources were created\n",
    "    \"\"\"\n",
    "    \n",
    "    cleanup_commands = f\"\"\"\n",
    "# List all compute instances\n",
    "gcloud compute instances list --project {project_id}\n",
    "\n",
    "# Delete specific instances\n",
    "gcloud compute instances delete clustrix-instance \\\n",
    "  --project {project_id} \\\n",
    "  --zone {zone} \\\n",
    "  --quiet\n",
    "\n",
    "# Delete managed instance groups\n",
    "gcloud compute instance-groups managed delete clustrix-preemptible-group \\\n",
    "  --project {project_id} \\\n",
    "  --zone {zone} \\\n",
    "  --quiet\n",
    "\n",
    "# Delete instance templates\n",
    "gcloud compute instance-templates delete clustrix-preemptible-template \\\n",
    "  --project {project_id} \\\n",
    "  --quiet\n",
    "\n",
    "# Delete GKE clusters\n",
    "gcloud container clusters delete clustrix-cluster \\\n",
    "  --project {project_id} \\\n",
    "  --zone {zone} \\\n",
    "  --quiet\n",
    "\n",
    "# Delete Cloud Storage buckets (BE CAREFUL - THIS DELETES ALL DATA)\n",
    "gsutil -m rm -r gs://{project_id}-clustrix-batch\n",
    "gsutil -m rm -r gs://{project_id}-vertex-models\n",
    "\n",
    "# Delete firewall rules\n",
    "gcloud compute firewall-rules delete clustrix-allow-ssh clustrix-internal \\\n",
    "  --project {project_id} \\\n",
    "  --quiet\n",
    "\n",
    "# Delete VPC network\n",
    "gcloud compute networks subnets delete clustrix-subnet \\\n",
    "  --project {project_id} \\\n",
    "  --region {region} \\\n",
    "  --quiet\n",
    "\n",
    "gcloud compute networks delete clustrix-vpc \\\n",
    "  --project {project_id} \\\n",
    "  --quiet\n",
    "\n",
    "# Delete service accounts\n",
    "gcloud iam service-accounts delete clustrix-compute@{project_id}.iam.gserviceaccount.com \\\n",
    "  --project {project_id} \\\n",
    "  --quiet\n",
    "\n",
    "gcloud iam service-accounts delete clustrix-batch-sa@{project_id}.iam.gserviceaccount.com \\\n",
    "  --project {project_id} \\\n",
    "  --quiet\n",
    "\n",
    "# List remaining billable resources\n",
    "gcloud compute instances list --project {project_id}\n",
    "gcloud compute disks list --project {project_id}\n",
    "gcloud compute addresses list --project {project_id}\n",
    "gcloud container clusters list --project {project_id}\n",
    "\"\"\"\n",
    "    \n",
    "    print(f\"GCP Resource Cleanup Commands for Project: {project_id}\")\n",
    "    print(cleanup_commands)\n",
    "    print(\"\\n⚠️  WARNING: Some commands will permanently delete resources and data!\")\n",
    "    print(\"Review each resource before deleting and ensure you have backups if needed.\")\n",
    "    print(\"\\n💡 TIP: Use 'gcloud compute instances stop' instead of 'delete' to preserve instances while stopping charges.\")\n",
    "    \n",
    "    return {\n",
    "        'project_id': project_id,\n",
    "        'zone': zone,\n",
    "        'region': region,\n",
    "        'cleanup_commands': cleanup_commands\n",
    "    }\n",
    "\n",
    "cleanup_info = cleanup_gcp_resources('your-project-id')\n",
    "print(\"\\nCleanup commands generated. Always verify resources before deletion!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-gcp-example",
   "metadata": {},
   "source": [
    "## Advanced Example: Distributed Scientific Computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scientific-computing-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cluster(cores=4, memory=\"8GB\", time=\"01:00:00\")\n",
    "def gcp_scientific_simulation(simulation_params, storage_config):\n",
    "    \"\"\"\n",
    "    Distributed scientific simulation using GCP infrastructure.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from scipy.integrate import odeint\n",
    "    from scipy.optimize import minimize\n",
    "    from google.cloud import storage\n",
    "    import pickle\n",
    "    import time\n",
    "    import matplotlib.pyplot as plt\n",
    "    import io\n",
    "    \n",
    "    def lorenz_system(state, t, sigma, rho, beta):\n",
    "        \"\"\"Lorenz attractor differential equations.\"\"\"\n",
    "        x, y, z = state\n",
    "        return [\n",
    "            sigma * (y - x),\n",
    "            x * (rho - z) - y,\n",
    "            x * y - beta * z\n",
    "        ]\n",
    "    \n",
    "    def simulate_lorenz(params, time_points):\n",
    "        \"\"\"Simulate Lorenz system with given parameters.\"\"\"\n",
    "        initial_state = [1.0, 1.0, 1.0]\n",
    "        solution = odeint(\n",
    "            lorenz_system, initial_state, time_points,\n",
    "            args=(params['sigma'], params['rho'], params['beta'])\n",
    "        )\n",
    "        return solution\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Parameter sweep\n",
    "    parameter_sets = simulation_params['parameter_sets']\n",
    "    time_points = np.linspace(0, simulation_params['max_time'], simulation_params['num_points'])\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, params in enumerate(parameter_sets):\n",
    "        # Run simulation\n",
    "        solution = simulate_lorenz(params, time_points)\n",
    "        \n",
    "        # Analyze results\n",
    "        x, y, z = solution[:, 0], solution[:, 1], solution[:, 2]\n",
    "        \n",
    "        analysis = {\n",
    "            'params': params,\n",
    "            'max_x': float(np.max(x)),\n",
    "            'min_x': float(np.min(x)),\n",
    "            'max_y': float(np.max(y)),\n",
    "            'min_y': float(np.min(y)),\n",
    "            'max_z': float(np.max(z)),\n",
    "            'min_z': float(np.min(z)),\n",
    "            'mean_energy': float(np.mean(x**2 + y**2 + z**2)),\n",
    "            'final_state': [float(x[-1]), float(y[-1]), float(z[-1])]\n",
    "        }\n",
    "        \n",
    "        results.append(analysis)\n",
    "        \n",
    "        # Create visualization for first few parameter sets\n",
    "        if i < 3:\n",
    "            fig = plt.figure(figsize=(12, 4))\n",
    "            \n",
    "            # Time series\n",
    "            plt.subplot(1, 3, 1)\n",
    "            plt.plot(time_points, x, label='X')\n",
    "            plt.plot(time_points, y, label='Y')\n",
    "            plt.plot(time_points, z, label='Z')\n",
    "            plt.xlabel('Time')\n",
    "            plt.ylabel('State')\n",
    "            plt.title(f'Lorenz System (σ={params[\"sigma\"]}, ρ={params[\"rho\"]}, β={params[\"beta\"]})')\n",
    "            plt.legend()\n",
    "            \n",
    "            # Phase space (X-Y)\n",
    "            plt.subplot(1, 3, 2)\n",
    "            plt.plot(x, y)\n",
    "            plt.xlabel('X')\n",
    "            plt.ylabel('Y')\n",
    "            plt.title('X-Y Phase Space')\n",
    "            \n",
    "            # Phase space (X-Z)\n",
    "            plt.subplot(1, 3, 3)\n",
    "            plt.plot(x, z)\n",
    "            plt.xlabel('X')\n",
    "            plt.ylabel('Z')\n",
    "            plt.title('X-Z Phase Space')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Save plot to Cloud Storage\n",
    "            if storage_config:\n",
    "                img_buffer = io.BytesIO()\n",
    "                plt.savefig(img_buffer, format='png', dpi=150, bbox_inches='tight')\n",
    "                img_buffer.seek(0)\n",
    "                \n",
    "                storage_client = storage.Client(project=storage_config['project_id'])\n",
    "                bucket = storage_client.bucket(storage_config['bucket_name'])\n",
    "                \n",
    "                plot_blob = bucket.blob(f\"plots/lorenz_simulation_{i}.png\")\n",
    "                plot_blob.upload_from_string(img_buffer.getvalue(), content_type='image/png')\n",
    "            \n",
    "            plt.close()\n",
    "    \n",
    "    computation_time = time.time() - start_time\n",
    "    \n",
    "    # Save detailed results to Cloud Storage\n",
    "    if storage_config:\n",
    "        storage_client = storage.Client(project=storage_config['project_id'])\n",
    "        bucket = storage_client.bucket(storage_config['bucket_name'])\n",
    "        \n",
    "        results_blob = bucket.blob(\"results/simulation_results.pkl\")\n",
    "        results_bytes = pickle.dumps({\n",
    "            'simulation_params': simulation_params,\n",
    "            'results': results,\n",
    "            'computation_time': computation_time,\n",
    "            'timestamp': time.time()\n",
    "        })\n",
    "        results_blob.upload_from_string(results_bytes)\n",
    "    \n",
    "    return {\n",
    "        'num_simulations': len(parameter_sets),\n",
    "        'computation_time': computation_time,\n",
    "        'average_energy': np.mean([r['mean_energy'] for r in results]),\n",
    "        'max_energy': max([r['mean_energy'] for r in results]),\n",
    "        'min_energy': min([r['mean_energy'] for r in results]),\n",
    "        'results_summary': results[:3],  # First 3 for brevity\n",
    "        'storage_location': f\"gs://{storage_config['bucket_name']}/results/\" if storage_config else None\n",
    "    }\n",
    "\n",
    "# Example usage:\n",
    "# simulation_config = {\n",
    "#     'parameter_sets': [\n",
    "#         {'sigma': 10.0, 'rho': 28.0, 'beta': 8.0/3.0},\n",
    "#         {'sigma': 10.0, 'rho': 24.0, 'beta': 8.0/3.0},\n",
    "#         {'sigma': 10.0, 'rho': 32.0, 'beta': 8.0/3.0},\n",
    "#     ],\n",
    "#     'max_time': 25.0,\n",
    "#     'num_points': 10000\n",
    "# }\n",
    "# \n",
    "# storage_config = {\n",
    "#     'project_id': 'your-project-id',\n",
    "#     'bucket_name': 'your-results-bucket'\n",
    "# }\n",
    "# \n",
    "# result = gcp_scientific_simulation(simulation_config, storage_config)\n",
    "# print(f\"Completed {result['num_simulations']} simulations in {result['computation_time']:.2f} seconds\")\n",
    "# print(f\"Average energy: {result['average_energy']:.4f}\")\n",
    "\n",
    "print(\"Advanced scientific computing example defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gcp-summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This tutorial covered:\n",
    "\n",
    "1. **Setup**: GCP authentication and Clustrix installation\n",
    "2. **Compute Engine**: Direct VM configuration and management\n",
    "3. **GKE Integration**: Kubernetes clusters for containerized workloads\n",
    "4. **Cloud Batch**: Managed job scheduling for large-scale processing\n",
    "5. **Cloud Storage**: Data management and result storage\n",
    "6. **Vertex AI**: Machine learning platform integration\n",
    "7. **Cost Optimization**: Preemptible VMs and cost management strategies\n",
    "8. **Security**: Best practices for secure deployment\n",
    "9. **Resource Management**: Proper cleanup procedures\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Set up your GCP credentials and test the basic configuration\n",
    "- Start with a simple Compute Engine instance for initial testing\n",
    "- Consider GKE for containerized workloads and auto-scaling\n",
    "- Explore Cloud Batch for large-scale batch processing\n",
    "- Implement proper monitoring and cost controls\n",
    "- Use preemptible VMs for cost-effective fault-tolerant workloads\n",
    "\n",
    "### GCP-Specific Advantages\n",
    "\n",
    "- **Preemptible/Spot VMs**: Exceptional cost savings (up to 80%)\n",
    "- **Google Kubernetes Engine**: Industry-leading managed Kubernetes\n",
    "- **Vertex AI**: Comprehensive ML platform with AutoML capabilities\n",
    "- **Global Network**: Superior network performance and global reach\n",
    "- **BigQuery Integration**: Seamless data analytics integration\n",
    "- **Sustained Use Discounts**: Automatic discounts for sustained usage\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [Google Cloud Compute Engine Documentation](https://cloud.google.com/compute/docs)\n",
    "- [Google Kubernetes Engine Documentation](https://cloud.google.com/kubernetes-engine/docs)\n",
    "- [Google Cloud Batch Documentation](https://cloud.google.com/batch/docs)\n",
    "- [Vertex AI Documentation](https://cloud.google.com/vertex-ai/docs)\n",
    "- [Google Cloud Storage Documentation](https://cloud.google.com/storage/docs)\n",
    "- [Clustrix Documentation](https://clustrix.readthedocs.io/)\n",
    "\n",
    "**Remember**: Always monitor your GCP costs and clean up resources when not in use. Use budget alerts and billing export to track spending!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}