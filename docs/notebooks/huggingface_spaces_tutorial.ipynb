{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "hf-title",
   "metadata": {},
   "source": [
    "# HuggingFace Spaces Tutorial\n",
    "\n",
    "This tutorial demonstrates how to use Clustrix with HuggingFace Spaces for ML model deployment and distributed computing.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ContextLab/clustrix/blob/master/docs/notebooks/huggingface_spaces_tutorial.ipynb)\n",
    "\n",
    "## Overview\n",
    "\n",
    "HuggingFace Spaces provides a unique platform for ML applications that integrates well with Clustrix:\n",
    "\n",
    "- **Gradio Apps**: Interactive web interfaces for ML models\n",
    "- **Streamlit Apps**: Data science web applications\n",
    "- **Static Spaces**: HTML/JS applications\n",
    "- **Docker Spaces**: Custom containerized applications\n",
    "- **GPU Support**: Hardware acceleration for compute-intensive tasks\n",
    "- **Persistent Storage**: Data storage across sessions\n",
    "- **Secrets Management**: Secure credential storage\n",
    "- **Community Hub**: Easy sharing and collaboration\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. HuggingFace account (free)\n",
    "2. HuggingFace Hub token for authentication\n",
    "3. Basic understanding of Gradio or Streamlit\n",
    "4. Git for version control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "installation",
   "metadata": {},
   "source": [
    "## Installation and Setup\n",
    "\n",
    "Install Clustrix with HuggingFace dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Clustrix with HuggingFace support\n",
    "!pip install clustrix huggingface_hub gradio streamlit transformers datasets\n",
    "\n",
    "# Import required libraries\n",
    "import clustrix\n",
    "from clustrix import cluster, configure\n",
    "from huggingface_hub import HfApi, Repository, login, upload_file\n",
    "import gradio as gr\n",
    "import streamlit as st\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hf-authentication",
   "metadata": {},
   "source": [
    "## HuggingFace Authentication Setup\n",
    "\n",
    "### Option 1: Interactive Login"
   ]
  },
  {
   "cell_type": "code",
   "id": "hf-login",
   "metadata": {},
   "outputs": [],
   "source": "# Login to HuggingFace (will prompt for token)\n# login()\n\n# Or set token as environment variable\n# os.environ['HUGGINGFACE_HUB_TOKEN'] = 'your-token-here'\n\n# Test authentication\ntry:\n    api = HfApi()\n    user_info = api.whoami()\n    print(f\"Successfully authenticated as: {user_info['name']}\")\nexcept Exception as e:\n    print(f\"Authentication failed: {e}\")"
  },
  {
   "cell_type": "markdown",
   "id": "whunllp7ite",
   "source": "**Get your token from [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)**",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "spaces-overview",
   "metadata": {},
   "source": [
    "## Method 1: Gradio Space with Clustrix Backend\n",
    "\n",
    "### Create a Gradio App with Distributed Computing"
   ]
  },
  {
   "cell_type": "code",
   "id": "gradio-app",
   "metadata": {},
   "outputs": [],
   "source": "def create_gradio_clustrix_app():\n    \"\"\"\n    Create a Gradio app that uses Clustrix for backend computations.\n    \"\"\"\n    \n    # This would typically be configured to point to your cluster\n    # For demo purposes, we'll use local execution\n    configure(\n        cluster_host=None,  # Local execution for demo\n        package_manager=\"auto\"\n    )\n    \n    @cluster(cores=2, memory=\"4GB\")\n    def distributed_model_training(dataset_size, model_type, n_estimators):\n        \"\"\"Train ML model using distributed computing.\"\"\"\n        import numpy as np\n        from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n        from sklearn.datasets import make_classification\n        from sklearn.model_selection import train_test_split, cross_val_score\n        from sklearn.metrics import accuracy_score, classification_report\n        import time\n        \n        start_time = time.time()\n        \n        # Generate synthetic dataset\n        X, y = make_classification(\n            n_samples=int(dataset_size),\n            n_features=20,\n            n_classes=3,\n            n_informative=15,\n            random_state=42\n        )\n        \n        X_train, X_test, y_train, y_test = train_test_split(\n            X, y, test_size=0.2, random_state=42\n        )\n        \n        # Select model\n        if model_type == \"Random Forest\":\n            model = RandomForestClassifier(\n                n_estimators=int(n_estimators),\n                random_state=42,\n                n_jobs=-1\n            )\n        else:  # Gradient Boosting\n            model = GradientBoostingClassifier(\n                n_estimators=int(n_estimators),\n                random_state=42\n            )\n        \n        # Train model\n        model.fit(X_train, y_train)\n        \n        # Evaluate\n        y_pred = model.predict(X_test)\n        accuracy = accuracy_score(y_test, y_pred)\n        \n        # Cross-validation\n        cv_scores = cross_val_score(model, X_train, y_train, cv=5)\n        \n        training_time = time.time() - start_time\n        \n        return {\n            'accuracy': accuracy,\n            'cv_mean': cv_scores.mean(),\n            'cv_std': cv_scores.std(),\n            'training_time': training_time,\n            'model_type': model_type,\n            'n_estimators': n_estimators,\n            'dataset_size': dataset_size,\n            'feature_importance': model.feature_importances_[:5].tolist()\n        }\n    \n    def train_model_interface(dataset_size, model_type, n_estimators):\n        \"\"\"Gradio interface function.\"\"\"\n        try:\n            # Run distributed training\n            result = distributed_model_training(dataset_size, model_type, n_estimators)\n            \n            # Format results for display\n            output = f\"\"\"\n**Training Results:**\n\n- **Model Type:** {result['model_type']}\n- **Dataset Size:** {result['dataset_size']:,} samples\n- **Number of Estimators:** {result['n_estimators']}\n- **Test Accuracy:** {result['accuracy']:.4f}\n- **CV Mean Score:** {result['cv_mean']:.4f} Â± {result['cv_std']:.4f}\n- **Training Time:** {result['training_time']:.2f} seconds\n\n**Top 5 Feature Importances:**\n{', '.join([f'{imp:.4f}' for imp in result['feature_importance']])}\n\n*Computation completed using Clustrix distributed computing.*\n\"\"\"\n            return output\n            \n        except Exception as e:\n            return f\"Error during training: {str(e)}\"\n    \n    # Create Gradio interface\n    interface = gr.Interface(\n        fn=train_model_interface,\n        inputs=[\n            gr.Slider(\n                minimum=1000,\n                maximum=50000,\n                value=10000,\n                step=1000,\n                label=\"Dataset Size\"\n            ),\n            gr.Radio(\n                choices=[\"Random Forest\", \"Gradient Boosting\"],\n                value=\"Random Forest\",\n                label=\"Model Type\"\n            ),\n            gr.Slider(\n                minimum=10,\n                maximum=200,\n                value=100,\n                step=10,\n                label=\"Number of Estimators\"\n            )\n        ],\n        outputs=gr.Markdown(label=\"Training Results\"),\n        title=\"Clustrix Distributed ML Training\",\n        description=\"Train machine learning models using Clustrix distributed computing backend.\",\n        article=\"\"\"\n        ### About This Demo\n        \n        This Gradio app demonstrates how to integrate Clustrix with HuggingFace Spaces \n        for distributed machine learning. The backend uses Clustrix to:\n        \n        - Distribute model training across multiple cores\n        - Perform cross-validation in parallel\n        - Handle large datasets efficiently\n        \n        **Note:** In a production deployment, Clustrix would be configured to use \n        remote compute clusters (AWS, Azure, GCP, etc.) for true distributed computing.\n        \"\"\",\n        theme=\"default\",\n        examples=[\n            [5000, \"Random Forest\", 50],\n            [20000, \"Gradient Boosting\", 100],\n            [10000, \"Random Forest\", 150]\n        ]\n    )\n    \n    return interface\n\n# Create the Gradio app\napp = create_gradio_clustrix_app()"
  },
  {
   "cell_type": "markdown",
   "id": "e87qq279i8w",
   "source": "**Use `app.launch()` to run the Gradio app locally.**",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "space-files",
   "metadata": {},
   "source": [
    "### Create Space Files Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-space-files",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_huggingface_space_files():\n",
    "    \"\"\"\n",
    "    Create the necessary files for a HuggingFace Space.\n",
    "    \"\"\"\n",
    "    \n",
    "    # app.py - Main Gradio application\n",
    "    app_py_content = '''\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Import clustrix if available, otherwise use local computation\n",
    "try:\n",
    "    from clustrix import cluster, configure\n",
    "    CLUSTRIX_AVAILABLE = True\n",
    "    \n",
    "    # Configure clustrix (would normally point to remote cluster)\n",
    "    configure(\n",
    "        cluster_host=None,  # Local execution in HF Spaces\n",
    "        package_manager=\"pip\"\n",
    "    )\n",
    "    \n",
    "    @cluster(cores=2, memory=\"4GB\")\n",
    "    def train_model_distributed(dataset_size, model_type, n_estimators):\n",
    "        return train_model_local(dataset_size, model_type, n_estimators)\n",
    "        \n",
    "except ImportError:\n",
    "    CLUSTRIX_AVAILABLE = False\n",
    "    def train_model_distributed(dataset_size, model_type, n_estimators):\n",
    "        return train_model_local(dataset_size, model_type, n_estimators)\n",
    "\n",
    "def train_model_local(dataset_size, model_type, n_estimators):\n",
    "    \"\"\"Local model training function.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Generate synthetic dataset\n",
    "    X, y = make_classification(\n",
    "        n_samples=int(dataset_size),\n",
    "        n_features=20,\n",
    "        n_classes=3,\n",
    "        n_informative=15,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Select model\n",
    "    if model_type == \"Random Forest\":\n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators=int(n_estimators),\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    else:  # Gradient Boosting\n",
    "        model = GradientBoostingClassifier(\n",
    "            n_estimators=int(n_estimators),\n",
    "            random_state=42\n",
    "        )\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Cross-validation (simplified for HF Spaces)\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=3)  # Reduced CV folds\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'training_time': training_time,\n",
    "        'model_type': model_type,\n",
    "        'n_estimators': n_estimators,\n",
    "        'dataset_size': dataset_size,\n",
    "        'feature_importance': model.feature_importances_[:5].tolist()\n",
    "    }\n",
    "\n",
    "def train_model_interface(dataset_size, model_type, n_estimators):\n",
    "    \"\"\"Gradio interface function.\"\"\"\n",
    "    try:\n",
    "        # Run training (distributed if clustrix available, local otherwise)\n",
    "        result = train_model_distributed(dataset_size, model_type, n_estimators)\n",
    "        \n",
    "        # Format results for display\n",
    "        backend_info = \"Clustrix Distributed\" if CLUSTRIX_AVAILABLE else \"Local Computation\"\n",
    "        \n",
    "        output = f\"\"\"\n",
    "**Training Results** ({backend_info}):\n",
    "\n",
    "- **Model Type:** {result['model_type']}\n",
    "- **Dataset Size:** {result['dataset_size']:,} samples\n",
    "- **Number of Estimators:** {result['n_estimators']}\n",
    "- **Test Accuracy:** {result['accuracy']:.4f}\n",
    "- **CV Mean Score:** {result['cv_mean']:.4f} Â± {result['cv_std']:.4f}\n",
    "- **Training Time:** {result['training_time']:.2f} seconds\n",
    "\n",
    "**Top 5 Feature Importances:**\n",
    "{', '.join([f'{imp:.4f}' for imp in result['feature_importance']])}\n",
    "\n",
    "*Backend: {backend_info}*\n",
    "\"\"\"\n",
    "        return output\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error during training: {str(e)}\"\n",
    "\n",
    "# Create Gradio interface\n",
    "demo = gr.Interface(\n",
    "    fn=train_model_interface,\n",
    "    inputs=[\n",
    "        gr.Slider(\n",
    "            minimum=1000,\n",
    "            maximum=20000,  # Reduced for HF Spaces limits\n",
    "            value=5000,\n",
    "            step=1000,\n",
    "            label=\"Dataset Size\"\n",
    "        ),\n",
    "        gr.Radio(\n",
    "            choices=[\"Random Forest\", \"Gradient Boosting\"],\n",
    "            value=\"Random Forest\",\n",
    "            label=\"Model Type\"\n",
    "        ),\n",
    "        gr.Slider(\n",
    "            minimum=10,\n",
    "            maximum=100,  # Reduced for HF Spaces\n",
    "            value=50,\n",
    "            step=10,\n",
    "            label=\"Number of Estimators\"\n",
    "        )\n",
    "    ],\n",
    "    outputs=gr.Markdown(label=\"Training Results\"),\n",
    "    title=\"Clustrix Distributed ML Training\",\n",
    "    description=\"Train machine learning models with optional Clustrix distributed computing backend.\",\n",
    "    article=\"\"\"\n",
    "    ### About This Demo\n",
    "    \n",
    "    This HuggingFace Space demonstrates integration between Clustrix and Gradio. \n",
    "    \n",
    "    **Features:**\n",
    "    - Interactive ML model training\n",
    "    - Automatic fallback to local computation\n",
    "    - Real-time results and performance metrics\n",
    "    \n",
    "    **Clustrix Integration:**\n",
    "    When properly configured, Clustrix can distribute computations across:\n",
    "    - AWS EC2, Batch, or ParallelCluster\n",
    "    - Azure VMs, Batch, or CycleCloud\n",
    "    - Google Cloud Compute Engine, GKE, or Batch\n",
    "    - On-premise SLURM, PBS, or SGE clusters\n",
    "    \n",
    "    Visit [Clustrix Documentation](https://clustrix.readthedocs.io/) for setup instructions.\n",
    "    \"\"\",\n",
    "    examples=[\n",
    "        [3000, \"Random Forest\", 30],\n",
    "        [8000, \"Gradient Boosting\", 50],\n",
    "        [5000, \"Random Forest\", 70]\n",
    "    ]\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()\n",
    "'''\n",
    "    \n",
    "    # requirements.txt\n",
    "    requirements_content = '''\n",
    "gradio==4.44.0\n",
    "numpy==1.24.3\n",
    "scikit-learn==1.3.0\n",
    "clustrix>=0.1.1\n",
    "'''\n",
    "    \n",
    "    # README.md\n",
    "    readme_content = '''\n",
    "---\n",
    "title: Clustrix Distributed ML Training\n",
    "emoji: ðŸš€\n",
    "colorFrom: blue\n",
    "colorTo: green\n",
    "sdk: gradio\n",
    "sdk_version: 4.44.0\n",
    "app_file: app.py\n",
    "pinned: false\n",
    "license: mit\n",
    "tags:\n",
    "- machine-learning\n",
    "- distributed-computing\n",
    "- clustrix\n",
    "- scikit-learn\n",
    "---\n",
    "\n",
    "# Clustrix Distributed ML Training\n",
    "\n",
    "This HuggingFace Space demonstrates how to integrate Clustrix distributed computing \n",
    "with Gradio for interactive machine learning applications.\n",
    "\n",
    "## Features\n",
    "\n",
    "- **Interactive Training**: Train ML models through a web interface\n",
    "- **Multiple Algorithms**: Support for Random Forest and Gradient Boosting\n",
    "- **Real-time Results**: See training progress and results immediately\n",
    "- **Distributed Backend**: Optional Clustrix integration for scaling\n",
    "\n",
    "## How It Works\n",
    "\n",
    "1. **Data Generation**: Creates synthetic classification datasets\n",
    "2. **Model Training**: Trains selected algorithm with specified parameters\n",
    "3. **Evaluation**: Performs cross-validation and test set evaluation\n",
    "4. **Results Display**: Shows metrics and feature importance\n",
    "\n",
    "## Clustrix Integration\n",
    "\n",
    "When Clustrix is properly configured, this app can distribute computations across:\n",
    "\n",
    "- **Cloud Platforms**: AWS, Azure, Google Cloud\n",
    "- **HPC Clusters**: SLURM, PBS/Torque, SGE\n",
    "- **Container Orchestration**: Kubernetes, Docker Swarm\n",
    "- **SSH Clusters**: Any SSH-accessible compute nodes\n",
    "\n",
    "## Usage\n",
    "\n",
    "1. Adjust the dataset size (1,000 - 20,000 samples)\n",
    "2. Select the model type (Random Forest or Gradient Boosting)\n",
    "3. Set the number of estimators (10 - 100)\n",
    "4. Click \"Submit\" to start training\n",
    "5. View results including accuracy, cross-validation scores, and timing\n",
    "\n",
    "## Local Development\n",
    "\n",
    "To run this app locally:\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "python app.py\n",
    "```\n",
    "\n",
    "## Learn More\n",
    "\n",
    "- [Clustrix Documentation](https://clustrix.readthedocs.io/)\n",
    "- [Gradio Documentation](https://gradio.app/docs/)\n",
    "- [HuggingFace Spaces](https://huggingface.co/docs/hub/spaces)\n",
    "'''\n",
    "    \n",
    "    files = {\n",
    "        'app.py': app_py_content.strip(),\n",
    "        'requirements.txt': requirements_content.strip(),\n",
    "        'README.md': readme_content.strip()\n",
    "    }\n",
    "    \n",
    "    print(\"HuggingFace Space Files:\")\n",
    "    print(\"========================\")\n",
    "    \n",
    "    for filename, content in files.items():\n",
    "        print(f\"\\n--- {filename} ---\")\n",
    "        print(content[:500] + \"...\" if len(content) > 500 else content)\n",
    "    \n",
    "    return files\n",
    "\n",
    "space_files = create_huggingface_space_files()\n",
    "print(\"\\nSpace files created. Upload these to create your HuggingFace Space.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deploy-space",
   "metadata": {},
   "source": [
    "### Deploy to HuggingFace Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deploy-to-spaces",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deploy_clustrix_space(username, space_name, space_files):\n",
    "    \"\"\"\n",
    "    Deploy Clustrix app to HuggingFace Spaces.\n",
    "    \n",
    "    Args:\n",
    "        username: Your HuggingFace username\n",
    "        space_name: Name for the new space\n",
    "        space_files: Dictionary of files to upload\n",
    "    \"\"\"\n",
    "    \n",
    "    # Commands to create and deploy the space\n",
    "    deployment_commands = f\"\"\"\n",
    "# Method 1: Using HuggingFace Hub (Recommended)\n",
    "\n",
    "# Create space via web interface first:\n",
    "# 1. Go to https://huggingface.co/new-space\n",
    "# 2. Choose username: {username}\n",
    "# 3. Space name: {space_name}\n",
    "# 4. License: MIT\n",
    "# 5. SDK: Gradio\n",
    "# 6. Hardware: CPU basic (free) or upgrade as needed\n",
    "\n",
    "# Then clone and upload files:\n",
    "git clone https://huggingface.co/spaces/{username}/{space_name}\n",
    "cd {space_name}\n",
    "\n",
    "# Copy your files (app.py, requirements.txt, README.md) to this directory\n",
    "\n",
    "git add .\n",
    "git commit -m \"Initial commit: Clustrix distributed ML training app\"\n",
    "git push\n",
    "\n",
    "# Method 2: Using Python API\n",
    "# (Run this in Python after authentication)\n",
    "\"\"\"\n",
    "    \n",
    "    python_deployment = f'''\n",
    "from huggingface_hub import HfApi, upload_file\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "# Initialize API\n",
    "api = HfApi()\n",
    "\n",
    "# Create space\n",
    "api.create_repo(\n",
    "    repo_id=\"{username}/{space_name}\",\n",
    "    repo_type=\"space\",\n",
    "    space_sdk=\"gradio\",\n",
    "    private=False\n",
    ")\n",
    "\n",
    "# Upload files\n",
    "space_files = {space_files}\n",
    "\n",
    "for filename, content in space_files.items():\n",
    "    with tempfile.NamedTemporaryFile(mode='w', suffix=f'_{filename}', delete=False) as f:\n",
    "        f.write(content)\n",
    "        temp_path = f.name\n",
    "    \n",
    "    upload_file(\n",
    "        path_or_fileobj=temp_path,\n",
    "        path_in_repo=filename,\n",
    "        repo_id=\"{username}/{space_name}\",\n",
    "        repo_type=\"space\",\n",
    "        commit_message=f\"Add {filename}\"\n",
    "    )\n",
    "    \n",
    "    os.unlink(temp_path)\n",
    "\n",
    "print(f\"Space deployed: https://huggingface.co/spaces/{username}/{space_name}\")\n",
    "'''\n",
    "    \n",
    "    print(\"HuggingFace Space Deployment:\")\n",
    "    print(\"==============================\")\n",
    "    print(deployment_commands)\n",
    "    print(\"\\nPython Deployment Code:\")\n",
    "    print(python_deployment)\n",
    "    \n",
    "    return {\n",
    "        'space_url': f'https://huggingface.co/spaces/{username}/{space_name}',\n",
    "        'deployment_commands': deployment_commands,\n",
    "        'python_code': python_deployment\n",
    "    }\n",
    "\n",
    "# Example deployment\n",
    "deployment_info = deploy_clustrix_space(\n",
    "    username='your-username',  # Replace with your HF username\n",
    "    space_name='clustrix-ml-training',\n",
    "    space_files=space_files\n",
    ")\n",
    "\n",
    "print(\"\\nDeployment instructions generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "streamlit-app",
   "metadata": {},
   "source": [
    "## Method 2: Streamlit Space with Clustrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streamlit-app-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_streamlit_clustrix_app():\n",
    "    \"\"\"\n",
    "    Create a Streamlit app template for HuggingFace Spaces.\n",
    "    \"\"\"\n",
    "    \n",
    "    streamlit_app_content = '''\n",
    "import streamlit as st\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import time\n",
    "\n",
    "# Import clustrix if available\n",
    "try:\n",
    "    from clustrix import cluster, configure\n",
    "    CLUSTRIX_AVAILABLE = True\n",
    "    configure(cluster_host=None, package_manager=\"pip\")\n",
    "except ImportError:\n",
    "    CLUSTRIX_AVAILABLE = False\n",
    "\n",
    "st.set_page_config(\n",
    "    page_title=\"Clustrix ML Dashboard\",\n",
    "    page_icon=\"ðŸš€\",\n",
    "    layout=\"wide\",\n",
    "    initial_sidebar_state=\"expanded\"\n",
    ")\n",
    "\n",
    "st.title(\"ðŸš€ Clustrix Distributed ML Dashboard\")\n",
    "st.markdown(\"\"\"\n",
    "This dashboard demonstrates machine learning with Clustrix distributed computing backend.\n",
    "\"\"\")\n",
    "\n",
    "# Sidebar controls\n",
    "st.sidebar.header(\"Configuration\")\n",
    "\n",
    "dataset_size = st.sidebar.slider(\n",
    "    \"Dataset Size\", \n",
    "    min_value=1000, \n",
    "    max_value=20000, \n",
    "    value=5000, \n",
    "    step=1000\n",
    ")\n",
    "\n",
    "n_features = st.sidebar.slider(\n",
    "    \"Number of Features\", \n",
    "    min_value=5, \n",
    "    max_value=50, \n",
    "    value=20, \n",
    "    step=5\n",
    ")\n",
    "\n",
    "n_estimators = st.sidebar.slider(\n",
    "    \"Number of Estimators\", \n",
    "    min_value=10, \n",
    "    max_value=200, \n",
    "    value=100, \n",
    "    step=10\n",
    ")\n",
    "\n",
    "max_depth = st.sidebar.slider(\n",
    "    \"Max Depth\", \n",
    "    min_value=3, \n",
    "    max_value=20, \n",
    "    value=10\n",
    ")\n",
    "\n",
    "# Backend selection\n",
    "backend = st.sidebar.radio(\n",
    "    \"Computation Backend\",\n",
    "    [\"Local\", \"Clustrix (if available)\"]\n",
    ")\n",
    "\n",
    "if CLUSTRIX_AVAILABLE and backend == \"Clustrix (if available)\":\n",
    "    @cluster(cores=2, memory=\"4GB\")\n",
    "    def train_model_clustrix(dataset_size, n_features, n_estimators, max_depth):\n",
    "        return train_model_local(dataset_size, n_features, n_estimators, max_depth)\n",
    "    \n",
    "    train_function = train_model_clustrix\n",
    "    backend_status = \"ðŸš€ Clustrix Distributed\"\n",
    "else:\n",
    "    train_function = lambda *args: train_model_local(*args)\n",
    "    backend_status = \"ðŸ’» Local Computation\"\n",
    "\n",
    "def train_model_local(dataset_size, n_features, n_estimators, max_depth):\n",
    "    \"\"\"Train model locally.\"\"\"\n",
    "    # Generate dataset\n",
    "    X, y = make_classification(\n",
    "        n_samples=dataset_size,\n",
    "        n_features=n_features,\n",
    "        n_classes=3,\n",
    "        n_informative=max(3, n_features // 2),\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    start_time = time.time()\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'X_test': X_test,\n",
    "        'y_test': y_test,\n",
    "        'y_pred': y_pred,\n",
    "        'accuracy': accuracy,\n",
    "        'training_time': training_time,\n",
    "        'feature_importance': model.feature_importances_\n",
    "    }\n",
    "\n",
    "# Main content\n",
    "col1, col2 = st.columns([2, 1])\n",
    "\n",
    "with col2:\n",
    "    st.markdown(f\"**Backend:** {backend_status}\")\n",
    "    st.markdown(f\"**Clustrix Available:** {'âœ…' if CLUSTRIX_AVAILABLE else 'âŒ'}\")\n",
    "\n",
    "if st.button(\"ðŸš€ Train Model\", type=\"primary\"):\n",
    "    with st.spinner(\"Training model...\"):\n",
    "        # Train model\n",
    "        result = train_function(dataset_size, n_features, n_estimators, max_depth)\n",
    "        \n",
    "        # Display results\n",
    "        col1, col2, col3 = st.columns(3)\n",
    "        \n",
    "        with col1:\n",
    "            st.metric(\"Accuracy\", f\"{result['accuracy']:.4f}\")\n",
    "        \n",
    "        with col2:\n",
    "            st.metric(\"Training Time\", f\"{result['training_time']:.2f}s\")\n",
    "        \n",
    "        with col3:\n",
    "            st.metric(\"Test Samples\", len(result['y_test']))\n",
    "        \n",
    "        # Feature importance plot\n",
    "        st.subheader(\"Feature Importance\")\n",
    "        importance_df = pd.DataFrame({\n",
    "            'Feature': [f'Feature {i}' for i in range(len(result['feature_importance']))],\n",
    "            'Importance': result['feature_importance']\n",
    "        }).sort_values('Importance', ascending=True)\n",
    "        \n",
    "        fig_importance = px.bar(\n",
    "            importance_df.tail(10), \n",
    "            x='Importance', \n",
    "            y='Feature',\n",
    "            title=\"Top 10 Feature Importances\",\n",
    "            orientation='h'\n",
    "        )\n",
    "        st.plotly_chart(fig_importance, use_container_width=True)\n",
    "        \n",
    "        # Confusion matrix\n",
    "        st.subheader(\"Confusion Matrix\")\n",
    "        cm = confusion_matrix(result['y_test'], result['y_pred'])\n",
    "        \n",
    "        fig_cm = px.imshow(\n",
    "            cm,\n",
    "            text_auto=True,\n",
    "            aspect=\"auto\",\n",
    "            title=\"Confusion Matrix\",\n",
    "            labels=dict(x=\"Predicted\", y=\"Actual\")\n",
    "        )\n",
    "        st.plotly_chart(fig_cm, use_container_width=True)\n",
    "\n",
    "# Information section\n",
    "st.markdown(\"---\")\n",
    "st.subheader(\"About Clustrix Integration\")\n",
    "\n",
    "col1, col2 = st.columns(2)\n",
    "\n",
    "with col1:\n",
    "    st.markdown(\"\"\"\n",
    "    **Clustrix Features:**\n",
    "    - ðŸŒ Distributed computing across clusters\n",
    "    - â˜ï¸ Cloud platform integration (AWS, Azure, GCP)\n",
    "    - ðŸ³ Container and Kubernetes support\n",
    "    - ðŸ“Š Automatic workload distribution\n",
    "    - ðŸ”§ Simple decorator-based API\n",
    "    \"\"\")\n",
    "\n",
    "with col2:\n",
    "    st.markdown(\"\"\"\n",
    "    **Supported Platforms:**\n",
    "    - AWS EC2, Batch, ParallelCluster\n",
    "    - Azure VMs, Batch, CycleCloud\n",
    "    - Google Compute Engine, GKE, Batch\n",
    "    - SLURM, PBS/Torque, SGE clusters\n",
    "    - SSH-accessible compute nodes\n",
    "    \"\"\")\n",
    "\n",
    "st.markdown(\"\"\"\n",
    "**Learn More:**\n",
    "- [Clustrix Documentation](https://clustrix.readthedocs.io/)\n",
    "- [GitHub Repository](https://github.com/ContextLab/clustrix)\n",
    "- [PyPI Package](https://pypi.org/project/clustrix/)\n",
    "\"\"\")\n",
    "'''\n",
    "    \n",
    "    streamlit_requirements = '''\n",
    "streamlit==1.28.0\n",
    "numpy==1.24.3\n",
    "pandas==2.0.3\n",
    "scikit-learn==1.3.0\n",
    "plotly==5.15.0\n",
    "clustrix>=0.1.1\n",
    "'''\n",
    "    \n",
    "    streamlit_readme = '''\n",
    "---\n",
    "title: Clustrix ML Dashboard\n",
    "emoji: ðŸ“Š\n",
    "colorFrom: purple\n",
    "colorTo: pink\n",
    "sdk: streamlit\n",
    "sdk_version: 1.28.0\n",
    "app_file: app.py\n",
    "pinned: false\n",
    "license: mit\n",
    "tags:\n",
    "- machine-learning\n",
    "- distributed-computing\n",
    "- clustrix\n",
    "- dashboard\n",
    "---\n",
    "\n",
    "# Clustrix ML Dashboard\n",
    "\n",
    "An interactive Streamlit dashboard demonstrating Clustrix distributed computing \n",
    "for machine learning workflows.\n",
    "\n",
    "## Features\n",
    "\n",
    "- ðŸ“Š **Interactive Dashboard**: Real-time model training and visualization\n",
    "- ðŸš€ **Distributed Computing**: Optional Clustrix backend for scaling\n",
    "- ðŸ“ˆ **Rich Visualizations**: Feature importance and confusion matrix plots\n",
    "- âš™ï¸ **Configurable Parameters**: Adjust dataset size, model parameters\n",
    "- ðŸ”„ **Backend Selection**: Choose between local and distributed computation\n",
    "\n",
    "## Usage\n",
    "\n",
    "1. Configure dataset and model parameters in the sidebar\n",
    "2. Select computation backend (local or Clustrix)\n",
    "3. Click \"Train Model\" to start training\n",
    "4. View results, metrics, and visualizations\n",
    "\n",
    "## Clustrix Integration\n",
    "\n",
    "When Clustrix is available and configured, this dashboard can distribute \n",
    "ML computations across various platforms for improved performance and scalability.\n",
    "'''\n",
    "    \n",
    "    return {\n",
    "        'app.py': streamlit_app_content.strip(),\n",
    "        'requirements.txt': streamlit_requirements.strip(),\n",
    "        'README.md': streamlit_readme.strip()\n",
    "    }\n",
    "\n",
    "streamlit_files = create_streamlit_clustrix_app()\n",
    "print(\"Streamlit app files created for HuggingFace Spaces deployment.\")\n",
    "print(\"\\nKey features:\")\n",
    "print(\"- Interactive dashboard with real-time training\")\n",
    "print(\"- Rich visualizations with Plotly\")\n",
    "print(\"- Configurable parameters and backend selection\")\n",
    "print(\"- Automatic fallback to local computation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gpu-spaces",
   "metadata": {},
   "source": [
    "## Method 3: GPU-Accelerated Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gpu-space-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gpu_clustrix_space():\n",
    "    \"\"\"\n",
    "    Create a GPU-accelerated HuggingFace Space with Clustrix.\n",
    "    \"\"\"\n",
    "    \n",
    "    gpu_app_content = '''\n",
    "import gradio as gr\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import time\n",
    "import json\n",
    "\n",
    "# Import clustrix if available\n",
    "try:\n",
    "    from clustrix import cluster, configure\n",
    "    CLUSTRIX_AVAILABLE = True\n",
    "    \n",
    "    # Configure for GPU-enabled remote clusters\n",
    "    configure(\n",
    "        cluster_host=None,  # Local for HF Spaces\n",
    "        package_manager=\"pip\",\n",
    "        default_cores=1,  # GPU tasks typically use 1 core\n",
    "        default_memory=\"8GB\"\n",
    "    )\n",
    "except ImportError:\n",
    "    CLUSTRIX_AVAILABLE = False\n",
    "\n",
    "# Check GPU availability\n",
    "CUDA_AVAILABLE = torch.cuda.is_available()\n",
    "device = \"cuda\" if CUDA_AVAILABLE else \"cpu\"\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Clustrix available: {CLUSTRIX_AVAILABLE}\")\n",
    "\n",
    "# Load a pre-trained model for demonstration\n",
    "@cluster(cores=1, memory=\"8GB\") if CLUSTRIX_AVAILABLE else (lambda f: f)\n",
    "def load_sentiment_model():\n",
    "    \"\"\"Load sentiment analysis model.\"\"\"\n",
    "    model_name = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "    \n",
    "    if CUDA_AVAILABLE:\n",
    "        model = model.to(device)\n",
    "    \n",
    "    return pipeline(\n",
    "        \"sentiment-analysis\", \n",
    "        model=model, \n",
    "        tokenizer=tokenizer, \n",
    "        device=0 if CUDA_AVAILABLE else -1\n",
    "    )\n",
    "\n",
    "# Initialize model\n",
    "sentiment_pipeline = load_sentiment_model()\n",
    "\n",
    "@cluster(cores=1, memory=\"4GB\") if CLUSTRIX_AVAILABLE else (lambda f: f)\n",
    "def batch_sentiment_analysis(texts, use_gpu=True):\n",
    "    \"\"\"Perform batch sentiment analysis.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Process texts in batches\n",
    "    batch_size = 16 if use_gpu and CUDA_AVAILABLE else 8\n",
    "    results = []\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        batch_results = sentiment_pipeline(batch)\n",
    "        results.extend(batch_results)\n",
    "    \n",
    "    processing_time = time.time() - start_time\n",
    "    \n",
    "    # Aggregate results\n",
    "    positive_count = sum(1 for r in results if r['label'] == 'LABEL_2')\n",
    "    negative_count = sum(1 for r in results if r['label'] == 'LABEL_0')\n",
    "    neutral_count = sum(1 for r in results if r['label'] == 'LABEL_1')\n",
    "    \n",
    "    avg_confidence = np.mean([r['score'] for r in results])\n",
    "    \n",
    "    return {\n",
    "        'results': results,\n",
    "        'summary': {\n",
    "            'total_texts': len(texts),\n",
    "            'positive': positive_count,\n",
    "            'negative': negative_count,\n",
    "            'neutral': neutral_count,\n",
    "            'avg_confidence': avg_confidence,\n",
    "            'processing_time': processing_time,\n",
    "            'texts_per_second': len(texts) / processing_time,\n",
    "            'device_used': device,\n",
    "            'clustrix_enabled': CLUSTRIX_AVAILABLE\n",
    "        }\n",
    "    }\n",
    "\n",
    "def process_text_input(text_input, sample_size):\n",
    "    \"\"\"Process text input for sentiment analysis.\"\"\"\n",
    "    try:\n",
    "        # Split text into individual texts\n",
    "        texts = [t.strip() for t in text_input.split('\\\\n') if t.strip()]\n",
    "        \n",
    "        # Limit sample size for demo\n",
    "        if len(texts) > sample_size:\n",
    "            texts = texts[:sample_size]\n",
    "        \n",
    "        if not texts:\n",
    "            return \"Please provide some text to analyze.\"\n",
    "        \n",
    "        # Run batch analysis\n",
    "        result = batch_sentiment_analysis(texts)\n",
    "        summary = result['summary']\n",
    "        \n",
    "        # Format output\n",
    "        output = f\"\"\"\n",
    "**Batch Sentiment Analysis Results**\n",
    "\n",
    "ðŸ“Š **Summary Statistics:**\n",
    "- Total texts analyzed: {summary['total_texts']}\n",
    "- Positive sentiment: {summary['positive']} ({summary['positive']/summary['total_texts']*100:.1f}%)\n",
    "- Negative sentiment: {summary['negative']} ({summary['negative']/summary['total_texts']*100:.1f}%)\n",
    "- Neutral sentiment: {summary['neutral']} ({summary['neutral']/summary['total_texts']*100:.1f}%)\n",
    "- Average confidence: {summary['avg_confidence']:.3f}\n",
    "\n",
    "âš¡ **Performance:**\n",
    "- Processing time: {summary['processing_time']:.2f} seconds\n",
    "- Throughput: {summary['texts_per_second']:.1f} texts/second\n",
    "- Device: {summary['device_used'].upper()}\n",
    "- Backend: {'Clustrix Distributed' if summary['clustrix_enabled'] else 'Local Processing'}\n",
    "\n",
    "ðŸ“ **Individual Results:**\n",
    "\"\"\"\n",
    "        \n",
    "        # Show first few individual results\n",
    "        for i, (text, result_item) in enumerate(zip(texts[:5], result['results'][:5])):\n",
    "            sentiment = {'LABEL_0': 'Negative', 'LABEL_1': 'Neutral', 'LABEL_2': 'Positive'}[result_item['label']]\n",
    "            confidence = result_item['score']\n",
    "            output += f\"\\n{i+1}. \\\"{text[:50]}{'...' if len(text) > 50 else ''}\\\" â†’ {sentiment} ({confidence:.3f})\"\n",
    "        \n",
    "        if len(texts) > 5:\n",
    "            output += f\"\\n... and {len(texts) - 5} more texts\"\n",
    "        \n",
    "        return output\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error during analysis: {str(e)}\"\n",
    "\n",
    "# Create Gradio interface\n",
    "demo = gr.Interface(\n",
    "    fn=process_text_input,\n",
    "    inputs=[\n",
    "        gr.Textbox(\n",
    "            lines=10,\n",
    "            placeholder=\"Enter texts to analyze (one per line)\\\\nExample:\\\\nI love this product!\\\\nThis is terrible.\\\\nIt's okay, nothing special.\",\n",
    "            label=\"Text Input\"\n",
    "        ),\n",
    "        gr.Slider(\n",
    "            minimum=1,\n",
    "            maximum=100,\n",
    "            value=20,\n",
    "            step=1,\n",
    "            label=\"Max Texts to Process\"\n",
    "        )\n",
    "    ],\n",
    "    outputs=gr.Markdown(label=\"Analysis Results\"),\n",
    "    title=\"ðŸš€ Clustrix GPU-Accelerated Sentiment Analysis\",\n",
    "    description=f\"\"\"\n",
    "    Batch sentiment analysis using transformer models with optional Clustrix distributed computing.\n",
    "    \n",
    "    **Current Setup:**\n",
    "    - Device: {device.upper()}\n",
    "    - Clustrix: {'âœ… Available' if CLUSTRIX_AVAILABLE else 'âŒ Not Available'}\n",
    "    - GPU Acceleration: {'âœ… Enabled' if CUDA_AVAILABLE else 'âŒ CPU Only'}\n",
    "    \"\"\",\n",
    "    article=\"\"\"\n",
    "    ### About This Demo\n",
    "    \n",
    "    This HuggingFace Space demonstrates GPU-accelerated NLP processing with Clustrix:\n",
    "    \n",
    "    **Features:**\n",
    "    - Batch processing of multiple texts\n",
    "    - GPU acceleration when available\n",
    "    - Comprehensive performance metrics\n",
    "    - Optional distributed computing backend\n",
    "    \n",
    "    **Clustrix Integration:**\n",
    "    In production, Clustrix can distribute GPU workloads across:\n",
    "    - Cloud GPU instances (AWS P3/P4, Azure NC/ND, GCP A100)\n",
    "    - Multi-GPU clusters with SLURM/PBS scheduling\n",
    "    - Kubernetes GPU nodes\n",
    "    - On-premise GPU clusters\n",
    "    \n",
    "    **Model:** `cardiffnlp/twitter-roberta-base-sentiment-latest`\n",
    "    \"\"\",\n",
    "    examples=[\n",
    "        [\n",
    "            \"I absolutely love this new feature!\\\\nThis is the worst experience ever.\\\\nIt's pretty good, could be better.\\\\nAmazing work by the team!\\\\nNot impressed at all.\",\n",
    "            5\n",
    "        ],\n",
    "        [\n",
    "            \"Great product, highly recommend!\\\\nTerrible customer service.\\\\nAverage quality for the price.\\\\nOutstanding performance!\\\\nWaste of money.\",\n",
    "            5\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()\n",
    "'''\n",
    "    \n",
    "    gpu_requirements = '''\n",
    "gradio==4.44.0\n",
    "torch==2.1.0\n",
    "transformers==4.35.0\n",
    "numpy==1.24.3\n",
    "clustrix>=0.1.1\n",
    "'''\n",
    "    \n",
    "    gpu_readme = '''\n",
    "---\n",
    "title: Clustrix GPU Sentiment Analysis\n",
    "emoji: âš¡\n",
    "colorFrom: yellow\n",
    "colorTo: orange\n",
    "sdk: gradio\n",
    "sdk_version: 4.44.0\n",
    "app_file: app.py\n",
    "pinned: false\n",
    "license: mit\n",
    "tags:\n",
    "- nlp\n",
    "- sentiment-analysis\n",
    "- gpu\n",
    "- distributed-computing\n",
    "- clustrix\n",
    "hardware: t4-small\n",
    "---\n",
    "\n",
    "# Clustrix GPU-Accelerated Sentiment Analysis\n",
    "\n",
    "A high-performance sentiment analysis demo showcasing GPU acceleration \n",
    "and Clustrix distributed computing integration.\n",
    "\n",
    "## Features\n",
    "\n",
    "- âš¡ **GPU Acceleration**: Utilizes GPU for faster inference\n",
    "- ðŸ“Š **Batch Processing**: Efficiently processes multiple texts\n",
    "- ðŸš€ **Clustrix Integration**: Optional distributed computing backend\n",
    "- ðŸ“ˆ **Performance Metrics**: Real-time throughput and timing\n",
    "- ðŸ¤– **Transformer Models**: Uses state-of-the-art RoBERTa model\n",
    "\n",
    "## Usage\n",
    "\n",
    "1. Enter multiple texts (one per line) in the input box\n",
    "2. Set the maximum number of texts to process\n",
    "3. Click \"Submit\" to run batch sentiment analysis\n",
    "4. View results including sentiment distribution and performance metrics\n",
    "\n",
    "## Model\n",
    "\n",
    "This demo uses `cardiffnlp/twitter-roberta-base-sentiment-latest`, \n",
    "a RoBERTa model fine-tuned for sentiment analysis on Twitter data.\n",
    "\n",
    "## Clustrix Scaling\n",
    "\n",
    "In production environments, Clustrix can distribute GPU workloads across:\n",
    "- Multi-GPU cloud instances\n",
    "- GPU clusters with job schedulers\n",
    "- Kubernetes GPU nodes\n",
    "- Hybrid cloud-edge deployments\n",
    "'''\n",
    "    \n",
    "    return {\n",
    "        'app.py': gpu_app_content.strip(),\n",
    "        'requirements.txt': gpu_requirements.strip(),\n",
    "        'README.md': gpu_readme.strip()\n",
    "    }\n",
    "\n",
    "gpu_files = create_gpu_clustrix_space()\n",
    "print(\"GPU-accelerated HuggingFace Space files created.\")\n",
    "print(\"\\nKey features:\")\n",
    "print(\"- GPU acceleration for transformer models\")\n",
    "print(\"- Batch processing for improved throughput\")\n",
    "print(\"- Real-time performance metrics\")\n",
    "print(\"- Clustrix integration for distributed GPU computing\")\n",
    "print(\"\\nNote: Requires GPU hardware tier on HuggingFace Spaces.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secrets-management",
   "metadata": {},
   "source": [
    "## Secrets and Configuration Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secrets-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_hf_spaces_secrets():\n",
    "    \"\"\"\n",
    "    Guide for managing secrets in HuggingFace Spaces for Clustrix integration.\n",
    "    \"\"\"\n",
    "    \n",
    "    secrets_guide = \"\"\"\n",
    "HuggingFace Spaces Secrets Management for Clustrix:\n",
    "\n",
    "1. **Access Secrets in Space Settings:**\n",
    "   - Go to your Space settings page\n",
    "   - Navigate to the \"Repository secrets\" section\n",
    "   - Add secrets as key-value pairs\n",
    "\n",
    "2. **Common Clustrix Secrets:**\n",
    "   - CLUSTER_HOST: IP address of your compute cluster\n",
    "   - CLUSTER_USERNAME: SSH username for cluster access\n",
    "   - CLUSTER_SSH_KEY: Private SSH key (base64 encoded)\n",
    "   - AWS_ACCESS_KEY_ID: AWS credentials for cloud clusters\n",
    "   - AWS_SECRET_ACCESS_KEY: AWS secret key\n",
    "   - AZURE_CLIENT_ID: Azure service principal ID\n",
    "   - AZURE_CLIENT_SECRET: Azure service principal secret\n",
    "   - GCP_SERVICE_ACCOUNT_KEY: Google Cloud service account JSON\n",
    "\n",
    "3. **Security Best Practices:**\n",
    "   - Use service accounts instead of personal credentials\n",
    "   - Rotate secrets regularly\n",
    "   - Apply principle of least privilege\n",
    "   - Monitor secret usage and access logs\n",
    "\n",
    "4. **Environment Variables in Code:**\n",
    "   Secrets are automatically available as environment variables\n",
    "\"\"\"\n",
    "    \n",
    "    config_code = '''\n",
    "import os\n",
    "import base64\n",
    "import tempfile\n",
    "from clustrix import configure\n",
    "\n",
    "def setup_clustrix_from_secrets():\n",
    "    \"\"\"Configure Clustrix using HuggingFace Spaces secrets.\"\"\"\n",
    "    \n",
    "    # Get cluster configuration from secrets\n",
    "    cluster_host = os.getenv('CLUSTER_HOST')\n",
    "    cluster_username = os.getenv('CLUSTER_USERNAME', 'clustrix')\n",
    "    ssh_key_b64 = os.getenv('CLUSTER_SSH_KEY')\n",
    "    \n",
    "    if not cluster_host:\n",
    "        print(\"No cluster host configured, using local execution\")\n",
    "        configure(cluster_host=None)\n",
    "        return False\n",
    "    \n",
    "    # Handle SSH key\n",
    "    key_file_path = None\n",
    "    if ssh_key_b64:\n",
    "        try:\n",
    "            # Decode base64 SSH key\n",
    "            ssh_key = base64.b64decode(ssh_key_b64).decode('utf-8')\n",
    "            \n",
    "            # Write to temporary file\n",
    "            with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.pem') as f:\n",
    "                f.write(ssh_key)\n",
    "                key_file_path = f.name\n",
    "            \n",
    "            # Set correct permissions\n",
    "            os.chmod(key_file_path, 0o600)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing SSH key: {e}\")\n",
    "            return False\n",
    "    \n",
    "    # Configure Clustrix\n",
    "    try:\n",
    "        configure(\n",
    "            cluster_type=\"ssh\",\n",
    "            cluster_host=cluster_host,\n",
    "            username=cluster_username,\n",
    "            key_file=key_file_path,\n",
    "            remote_work_dir=\"/tmp/clustrix\",\n",
    "            package_manager=\"auto\",\n",
    "            default_cores=2,\n",
    "            default_memory=\"4GB\",\n",
    "            default_time=\"01:00:00\"\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ… Clustrix configured for remote execution on {cluster_host}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to configure Clustrix: {e}\")\n",
    "        configure(cluster_host=None)  # Fallback to local\n",
    "        return False\n",
    "\n",
    "def setup_cloud_credentials():\n",
    "    \"\"\"Setup cloud credentials from secrets.\"\"\"\n",
    "    \n",
    "    # AWS credentials\n",
    "    aws_key = os.getenv('AWS_ACCESS_KEY_ID')\n",
    "    aws_secret = os.getenv('AWS_SECRET_ACCESS_KEY')\n",
    "    if aws_key and aws_secret:\n",
    "        os.environ['AWS_ACCESS_KEY_ID'] = aws_key\n",
    "        os.environ['AWS_SECRET_ACCESS_KEY'] = aws_secret\n",
    "        print(\"âœ… AWS credentials configured\")\n",
    "    \n",
    "    # Azure credentials\n",
    "    azure_client_id = os.getenv('AZURE_CLIENT_ID')\n",
    "    azure_client_secret = os.getenv('AZURE_CLIENT_SECRET')\n",
    "    azure_tenant_id = os.getenv('AZURE_TENANT_ID')\n",
    "    if azure_client_id and azure_client_secret and azure_tenant_id:\n",
    "        os.environ['AZURE_CLIENT_ID'] = azure_client_id\n",
    "        os.environ['AZURE_CLIENT_SECRET'] = azure_client_secret\n",
    "        os.environ['AZURE_TENANT_ID'] = azure_tenant_id\n",
    "        print(\"âœ… Azure credentials configured\")\n",
    "    \n",
    "    # Google Cloud credentials\n",
    "    gcp_key = os.getenv('GCP_SERVICE_ACCOUNT_KEY')\n",
    "    if gcp_key:\n",
    "        with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.json') as f:\n",
    "            f.write(gcp_key)\n",
    "            os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = f.name\n",
    "        print(\"âœ… Google Cloud credentials configured\")\n",
    "\n",
    "# Usage in your Space app:\n",
    "if __name__ == \"__main__\":\n",
    "    setup_cloud_credentials()\n",
    "    clustrix_enabled = setup_clustrix_from_secrets()\n",
    "    \n",
    "    # Your app code here\n",
    "    print(f\"Clustrix distributed computing: {'Enabled' if clustrix_enabled else 'Disabled (local mode)'}\")\n",
    "'''\n",
    "    \n",
    "    print(\"HuggingFace Spaces Secrets Management Guide:\")\n",
    "    print(\"===========================================\")\n",
    "    print(secrets_guide)\n",
    "    print(\"\\nConfiguration Code Example:\")\n",
    "    print(config_code)\n",
    "    \n",
    "    return {\n",
    "        'guide': secrets_guide,\n",
    "        'config_code': config_code\n",
    "    }\n",
    "\n",
    "secrets_info = setup_hf_spaces_secrets()\n",
    "print(\"\\nSecrets management guide created for secure Clustrix integration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deployment-tips",
   "metadata": {},
   "source": [
    "## Deployment Tips and Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deployment-best-practices",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hf_spaces_best_practices():\n",
    "    \"\"\"\n",
    "    Best practices for deploying Clustrix apps on HuggingFace Spaces.\n",
    "    \"\"\"\n",
    "    \n",
    "    best_practices = \"\"\"\n",
    "HuggingFace Spaces + Clustrix Best Practices:\n",
    "\n",
    "ðŸš€ **Performance Optimization:**\n",
    "- Use appropriate hardware tier (CPU Basic â†’ T4 Small â†’ A10G Small)\n",
    "- Implement caching for models and data\n",
    "- Use batch processing for multiple requests\n",
    "- Optimize memory usage with careful tensor management\n",
    "- Consider async processing for long-running tasks\n",
    "\n",
    "ðŸ”’ **Security:**\n",
    "- Store all credentials in Spaces secrets\n",
    "- Use service accounts instead of personal credentials\n",
    "- Implement input validation and sanitization\n",
    "- Never log sensitive information\n",
    "- Use HTTPS for all external API calls\n",
    "\n",
    "ðŸŽ¯ **User Experience:**\n",
    "- Provide clear error messages and fallbacks\n",
    "- Show progress indicators for long operations\n",
    "- Include example inputs and use cases\n",
    "- Add comprehensive documentation\n",
    "- Implement graceful degradation when Clustrix is unavailable\n",
    "\n",
    "ðŸ“Š **Monitoring and Debugging:**\n",
    "- Add logging for key operations\n",
    "- Include performance metrics in the UI\n",
    "- Monitor resource usage and costs\n",
    "- Set up alerts for failures\n",
    "- Use descriptive commit messages for versioning\n",
    "\n",
    "ðŸ”„ **Scalability:**\n",
    "- Design for both local and distributed execution\n",
    "- Implement proper error handling and retries\n",
    "- Use connection pooling for database/API connections\n",
    "- Consider rate limiting for external services\n",
    "- Plan for traffic spikes and scaling needs\n",
    "\n",
    "ðŸ“¦ **Deployment:**\n",
    "- Pin specific package versions in requirements.txt\n",
    "- Test locally before deploying\n",
    "- Use environment variables for configuration\n",
    "- Implement health checks and status endpoints\n",
    "- Document deployment process and dependencies\n",
    "\"\"\"\n",
    "    \n",
    "    troubleshooting = \"\"\"\n",
    "Common Issues and Solutions:\n",
    "\n",
    "âŒ **Problem: Space fails to start**\n",
    "âœ… **Solution:**\n",
    "   - Check requirements.txt for version conflicts\n",
    "   - Verify Python version compatibility\n",
    "   - Review app.py for syntax errors\n",
    "   - Check Space logs for detailed error messages\n",
    "\n",
    "âŒ **Problem: Clustrix connection fails**\n",
    "âœ… **Solution:**\n",
    "   - Verify cluster host is accessible from HF Spaces\n",
    "   - Check SSH key format and permissions\n",
    "   - Ensure firewall allows connections from HF IPs\n",
    "   - Implement fallback to local execution\n",
    "\n",
    "âŒ **Problem: GPU not detected**\n",
    "âœ… **Solution:**\n",
    "   - Upgrade to GPU-enabled hardware tier\n",
    "   - Check torch.cuda.is_available() in code\n",
    "   - Verify CUDA-compatible PyTorch version\n",
    "   - Add GPU requirements to README hardware field\n",
    "\n",
    "âŒ **Problem: Memory errors**\n",
    "âœ… **Solution:**\n",
    "   - Optimize batch sizes for available memory\n",
    "   - Clear GPU cache with torch.cuda.empty_cache()\n",
    "   - Use memory-efficient model loading\n",
    "   - Consider model quantization or distillation\n",
    "\n",
    "âŒ **Problem: Slow performance**\n",
    "âœ… **Solution:**\n",
    "   - Profile code to identify bottlenecks\n",
    "   - Use appropriate hardware tier\n",
    "   - Implement model caching and warm-up\n",
    "   - Optimize data preprocessing pipeline\n",
    "\"\"\"\n",
    "    \n",
    "    hardware_guide = \"\"\"\n",
    "HuggingFace Spaces Hardware Tiers:\n",
    "\n",
    "ðŸ†“ **CPU Basic (Free):**\n",
    "   - 2 vCPUs, 16GB RAM\n",
    "   - Good for: Simple demos, small models, prototyping\n",
    "   - Clustrix use case: Local fallback, lightweight computations\n",
    "\n",
    "ðŸ’° **CPU Upgrade ($3/hour):**\n",
    "   - 8 vCPUs, 32GB RAM\n",
    "   - Good for: CPU-intensive tasks, larger datasets\n",
    "   - Clustrix use case: Medium-scale local processing\n",
    "\n",
    "ðŸš€ **T4 Small ($0.60/hour):**\n",
    "   - 4 vCPUs, 15GB RAM, 1x T4 GPU (16GB VRAM)\n",
    "   - Good for: Deep learning inference, computer vision\n",
    "   - Clustrix use case: GPU-accelerated ML, model training demos\n",
    "\n",
    "âš¡ **A10G Small ($3.15/hour):**\n",
    "   - 4 vCPUs, 15GB RAM, 1x A10G GPU (24GB VRAM)\n",
    "   - Good for: Large models, high-performance inference\n",
    "   - Clustrix use case: Production-scale ML applications\n",
    "\n",
    "ðŸ”¥ **A100 Large ($4.13/hour):**\n",
    "   - 12 vCPUs, 46GB RAM, 1x A100 GPU (40GB VRAM)\n",
    "   - Good for: Massive models, research applications\n",
    "   - Clustrix use case: Distributed training coordination\n",
    "\"\"\"\n",
    "    \n",
    "    print(\"HuggingFace Spaces Best Practices:\")\n",
    "    print(\"===================================\")\n",
    "    print(best_practices)\n",
    "    print(\"\\nTroubleshooting Guide:\")\n",
    "    print(\"=====================\")\n",
    "    print(troubleshooting)\n",
    "    print(\"\\nHardware Selection Guide:\")\n",
    "    print(\"========================\")\n",
    "    print(hardware_guide)\n",
    "    \n",
    "    return {\n",
    "        'best_practices': best_practices,\n",
    "        'troubleshooting': troubleshooting,\n",
    "        'hardware_guide': hardware_guide\n",
    "    }\n",
    "\n",
    "practices_info = hf_spaces_best_practices()\n",
    "print(\"\\nComprehensive deployment guide created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hf-summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This tutorial covered:\n",
    "\n",
    "1. **Gradio Integration**: Interactive ML training interfaces with Clustrix backend\n",
    "2. **Streamlit Dashboards**: Rich data science applications with distributed computing\n",
    "3. **GPU Acceleration**: High-performance NLP processing with transformer models\n",
    "4. **Secrets Management**: Secure credential storage and configuration\n",
    "5. **Deployment Best Practices**: Performance optimization and troubleshooting\n",
    "6. **Hardware Selection**: Choosing appropriate tiers for different use cases\n",
    "\n",
    "### Key Advantages of HuggingFace Spaces + Clustrix\n",
    "\n",
    "- **Easy Deployment**: Simple git-based deployment workflow\n",
    "- **Community Sharing**: Built-in discoverability and collaboration\n",
    "- **Flexible Hardware**: From free CPU to high-end GPU instances\n",
    "- **Hybrid Computing**: Local execution with optional distributed scaling\n",
    "- **ML Focus**: Optimized for machine learning and AI applications\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Create your HuggingFace account and get an access token\n",
    "2. Start with a simple Gradio app using the provided templates\n",
    "3. Configure Clustrix integration using Spaces secrets\n",
    "4. Test locally before deploying to ensure compatibility\n",
    "5. Monitor performance and scale hardware as needed\n",
    "\n",
    "### Use Cases\n",
    "\n",
    "- **Research Demos**: Showcase distributed computing research\n",
    "- **Educational Tools**: Interactive learning environments\n",
    "- **Prototype Testing**: Rapid prototyping with real user feedback\n",
    "- **Model Serving**: Production-ready ML model deployment\n",
    "- **Collaborative Computing**: Shared access to distributed resources\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [HuggingFace Spaces Documentation](https://huggingface.co/docs/hub/spaces)\n",
    "- [Gradio Documentation](https://gradio.app/docs/)\n",
    "- [Streamlit Documentation](https://docs.streamlit.io/)\n",
    "- [Clustrix Documentation](https://clustrix.readthedocs.io/)\n",
    "- [HuggingFace Hub Python Library](https://huggingface.co/docs/huggingface_hub/)\n",
    "\n",
    "**Remember**: HuggingFace Spaces provides an excellent platform for showcasing Clustrix capabilities and building interactive ML applications with distributed computing backends!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}