{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aws-title",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amazon Web Services (AWS) Cloud Tutorial\n",
    "\n",
    "This tutorial demonstrates how to use Clustrix with Amazon Web Services (AWS) cloud infrastructure for scalable distributed computing.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ContextLab/clustrix/blob/master/docs/source/notebooks/aws_cloud_tutorial.ipynb)\n",
    "\n",
    "## Overview\n",
    "\n",
    "AWS provides several services that work well with Clustrix:\n",
    "\n",
    "- **EC2**: Virtual machines for compute clusters\n",
    "- **AWS Batch**: Managed job scheduling service\n",
    "- **ECS**: Container orchestration\n",
    "- **ParallelCluster**: HPC cluster management\n",
    "- **S3**: Object storage for data and results\n",
    "- **VPC**: Network isolation and security\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before starting this tutorial, ensure you have:\n",
    "\n",
    "1. **AWS Account**: Active AWS account with billing enabled\n",
    "2. **AWS CLI**: Installed and configured on your local machine\n",
    "3. **SSH Key Pair**: Generated and uploaded to AWS EC2 for secure access\n",
    "4. **IAM Permissions**: Appropriate permissions for EC2, S3, and other services\n",
    "5. **Basic AWS Knowledge**: Understanding of AWS services, regions, and availability zones\n",
    "6. **Python Environment**: Python 3.7+ with pip installed\n",
    "\n",
    "## Complete AWS Setup Guide\n",
    "\n",
    "### Step 1: Create AWS Account\n",
    "1. Go to [aws.amazon.com](https://aws.amazon.com) and create an account\n",
    "2. Verify your email and provide payment information\n",
    "3. Choose the Basic Support plan (free)\n",
    "\n",
    "### Step 2: Install AWS CLI\n",
    "```bash\n",
    "# On macOS\n",
    "brew install awscli\n",
    "\n",
    "# On Linux/WSL\n",
    "curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\n",
    "unzip awscliv2.zip\n",
    "sudo ./aws/install\n",
    "\n",
    "# On Windows\n",
    "# Download and run the AWS CLI MSI installer from AWS documentation\n",
    "```\n",
    "\n",
    "### Step 3: Create IAM User and Access Keys\n",
    "1. Go to AWS Console → IAM → Users → Create User\n",
    "2. Create a user with programmatic access\n",
    "3. Attach policies: `AmazonEC2FullAccess`, `AmazonS3FullAccess`, `IAMReadOnlyAccess`\n",
    "4. Save the Access Key ID and Secret Access Key securely\n",
    "\n",
    "### Step 4: Generate SSH Key Pair\n",
    "```bash\n",
    "# Generate SSH key pair locally\n",
    "ssh-keygen -t rsa -b 4096 -f ~/.ssh/aws-clustrix-key\n",
    "\n",
    "# Import public key to AWS\n",
    "aws ec2 import-key-pair --key-name \"clustrix-key\" --public-key-material fileb://~/.ssh/aws-clustrix-key.pub\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "installation",
   "metadata": {},
   "source": [
    "## Installation and Setup\n",
    "\n",
    "Install Clustrix with AWS dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Clustrix with AWS support\n",
    "!pip install clustrix boto3 awscli\n",
    "\n",
    "# Import required libraries\n",
    "import clustrix\n",
    "from clustrix import cluster, configure\n",
    "import boto3\n",
    "import os\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aws-credentials",
   "metadata": {},
   "outputs": [],
   "source": [
    "## AWS Credentials Configuration\n",
    "\n",
    "Configure your AWS credentials using one of the following methods:\n",
    "\n",
    "### Option 1: AWS CLI Configuration (Recommended)\n",
    "\n",
    "Run the following command in your terminal to configure credentials interactively:\n",
    "\n",
    "```bash\n",
    "aws configure\n",
    "```\n",
    "\n",
    "You'll be prompted to enter:\n",
    "- AWS Access Key ID\n",
    "- AWS Secret Access Key  \n",
    "- Default region name (e.g., us-east-1)\n",
    "- Default output format (json)\n",
    "\n",
    "This creates credential files at `~/.aws/credentials` and `~/.aws/config`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aws-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure AWS CLI (run this in terminal)\n",
    "# aws configure\n",
    "\n",
    "# Verify configuration\n",
    "!aws sts get-caller-identity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aws-creds-env",
   "metadata": {},
   "source": [
    "### Option 2: Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "env-vars",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Set AWS credentials as environment variables (if needed)\n",
    "# os.environ['AWS_ACCESS_KEY_ID'] = 'your-access-key'\n",
    "# os.environ['AWS_SECRET_ACCESS_KEY'] = 'your-secret-key'\n",
    "# os.environ['AWS_DEFAULT_REGION'] = 'us-east-1'\n",
    "\n",
    "# Test AWS connection\n",
    "try:\n",
    "    ec2 = boto3.client('ec2')\n",
    "    regions = ec2.describe_regions()\n",
    "    print(f\"✓ Successfully connected to AWS. Available regions: {len(regions['Regions'])}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ AWS connection failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Method 1: Direct EC2 Instance Configuration\n",
    "\n",
    "### Prerequisites: Create Security Group\n",
    "\n",
    "Before launching an EC2 instance, you need to create a security group that allows SSH access. You can do this through the AWS Console or use the function provided in the Security section below.\n",
    "\n",
    "**Quick Setup via AWS Console:**\n",
    "1. Go to EC2 → Security Groups → Create Security Group\n",
    "2. Name: `clustrix-sg`\n",
    "3. Add inbound rule: SSH (port 22) from your IP address only\n",
    "4. Note the Security Group ID (sg-xxxxxxxxx)\n",
    "\n",
    "### Launch EC2 Instance for Clustrix\n",
    "\n",
    "This example shows how to programmatically launch an EC2 instance suitable for Clustrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2-launch",
   "metadata": {},
   "outputs": [],
   "source": [
    "def launch_clustrix_ec2_instance(key_name, security_group_id, instance_type='t3.large'):\n",
    "    \"\"\"\n",
    "    Launch an EC2 instance configured for Clustrix.\n",
    "    \n",
    "    Args:\n",
    "        key_name: Name of your EC2 key pair\n",
    "        security_group_id: Security group ID that allows SSH access\n",
    "        instance_type: EC2 instance type\n",
    "    \n",
    "    Returns:\n",
    "        Instance ID and public IP\n",
    "    \"\"\"\n",
    "    ec2 = boto3.client('ec2')\n",
    "    \n",
    "    # User data script to setup Python environment\n",
    "    user_data = '''\n",
    "#!/bin/bash\n",
    "yum update -y\n",
    "yum install -y python3 python3-pip git\n",
    "pip3 install clustrix numpy scipy pandas\n",
    "\n",
    "# Install uv for faster package management\n",
    "curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "source $HOME/.cargo/env\n",
    "\n",
    "# Create clustrix user\n",
    "useradd -m -s /bin/bash clustrix\n",
    "mkdir -p /home/clustrix/.ssh\n",
    "cp /home/ec2-user/.ssh/authorized_keys /home/clustrix/.ssh/\n",
    "chown -R clustrix:clustrix /home/clustrix/.ssh\n",
    "chmod 700 /home/clustrix/.ssh\n",
    "chmod 600 /home/clustrix/.ssh/authorized_keys\n",
    "\n",
    "# Setup sudo access\n",
    "echo \"clustrix ALL=(ALL) NOPASSWD:ALL\" >> /etc/sudoers\n",
    "'''\n",
    "    \n",
    "    try:\n",
    "        response = ec2.run_instances(\n",
    "            ImageId='ami-0c02fb55956c7d316',  # Amazon Linux 2 AMI\n",
    "            MinCount=1,\n",
    "            MaxCount=1,\n",
    "            InstanceType=instance_type,\n",
    "            KeyName=key_name,\n",
    "            SecurityGroupIds=[security_group_id],\n",
    "            UserData=user_data,\n",
    "            TagSpecifications=[\n",
    "                {\n",
    "                    'ResourceType': 'instance',\n",
    "                    'Tags': [\n",
    "                        {'Key': 'Name', 'Value': 'Clustrix-Compute-Node'},\n",
    "                        {'Key': 'Purpose', 'Value': 'Clustrix-Tutorial'}\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        instance_id = response['Instances'][0]['InstanceId']\n",
    "        \n",
    "        # Wait for instance to be running\n",
    "        waiter = ec2.get_waiter('instance_running')\n",
    "        waiter.wait(InstanceIds=[instance_id])\n",
    "        \n",
    "        # Get public IP\n",
    "        instance_info = ec2.describe_instances(InstanceIds=[instance_id])\n",
    "        public_ip = instance_info['Reservations'][0]['Instances'][0].get('PublicIpAddress')\n",
    "        \n",
    "        return instance_id, public_ip\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error launching instance: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Example usage (uncomment and modify with your details)\n",
    "# instance_id, public_ip = launch_clustrix_ec2_instance(\n",
    "#     key_name='clustrix-key',\n",
    "#     security_group_id='sg-xxxxxxxxx'\n",
    "# )\n",
    "# \n",
    "# if instance_id and public_ip:\n",
    "#     print(f\"✓ Instance launched: {instance_id}\")\n",
    "#     print(f\"✓ Public IP: {public_ip}\")\n",
    "#     print(\"⏳ Wait 2-3 minutes for user data script to complete before connecting.\")\n",
    "# else:\n",
    "#     print(\"✗ Failed to launch instance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clustrix-config",
   "metadata": {},
   "source": [
    "### Configure Clustrix for EC2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config-ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Clustrix to use your EC2 instance\n",
    "configure(\n",
    "    cluster_type=\"ssh\",\n",
    "    cluster_host=\"your-ec2-public-ip\",  # Replace with actual IP\n",
    "    username=\"clustrix\",  # or \"ec2-user\" if using default user\n",
    "    key_file=\"~/.ssh/your-key.pem\",  # Path to your private key\n",
    "    remote_work_dir=\"/tmp/clustrix\",\n",
    "    package_manager=\"auto\",  # Will use uv if available, fallback to pip\n",
    "    default_cores=4,\n",
    "    default_memory=\"8GB\",\n",
    "    default_time=\"01:00:00\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fms2rlxukv8",
   "metadata": {},
   "outputs": [],
   "source": [
    "**Configuration Complete!** \n",
    "\n",
    "Your Clustrix is now configured to use the EC2 instance. Make sure to replace `your-ec2-public-ip` with the actual IP address of your running EC2 instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example-computation",
   "metadata": {},
   "source": [
    "### Example: Remote Computation on EC2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cluster(cores=2, memory=\"4GB\")\n",
    "def aws_monte_carlo_pi(n_samples=1000000):\n",
    "    \"\"\"Estimate π using Monte Carlo method on AWS EC2.\"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    # Generate random points\n",
    "    x = np.random.uniform(-1, 1, n_samples)\n",
    "    y = np.random.uniform(-1, 1, n_samples)\n",
    "    \n",
    "    # Count points inside unit circle\n",
    "    inside_circle = (x**2 + y**2) <= 1\n",
    "    pi_estimate = 4 * np.sum(inside_circle) / n_samples\n",
    "    \n",
    "    return {\n",
    "        'pi_estimate': pi_estimate,\n",
    "        'n_samples': n_samples,\n",
    "        'error': abs(pi_estimate - np.pi)\n",
    "    }\n",
    "\n",
    "# Example usage (uncomment to run on your EC2 instance):\n",
    "# result = aws_monte_carlo_pi(n_samples=5000000)\n",
    "# print(f\"π estimate: {result['pi_estimate']:.6f}\")\n",
    "# print(f\"Error: {result['error']:.6f}\")\n",
    "# print(f\"Samples used: {result['n_samples']:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s0rz170o8cq",
   "metadata": {},
   "outputs": [],
   "source": [
    "**Ready to Run!** \n",
    "\n",
    "The Monte Carlo π estimation function is now defined and ready to execute on your EC2 instance. Simply uncomment the example usage lines above to run the computation remotely on AWS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aws-batch",
   "metadata": {},
   "source": [
    "## Method 2: AWS Batch Configuration\n",
    "\n",
    "AWS Batch provides managed job scheduling for more complex workloads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "batch-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_aws_batch_environment():\n",
    "    \"\"\"\n",
    "    Example of setting up AWS Batch compute environment.\n",
    "    This is a template - you'll need to adapt it to your specific needs.\n",
    "    \"\"\"\n",
    "    batch = boto3.client('batch')\n",
    "    ec2 = boto3.client('ec2')\n",
    "    iam = boto3.client('iam')\n",
    "    \n",
    "    # This is a simplified example - real setup requires:\n",
    "    # 1. VPC and subnet configuration\n",
    "    # 2. IAM roles and policies\n",
    "    # 3. Security groups\n",
    "    # 4. Compute environment\n",
    "    # 5. Job queue\n",
    "    # 6. Job definition\n",
    "    \n",
    "    return {\n",
    "        'compute_environment': 'clustrix-batch-env',\n",
    "        'job_queue': 'clustrix-queue',\n",
    "        'job_definition': 'clustrix-job-def'\n",
    "    }\n",
    "\n",
    "# batch_config = create_aws_batch_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7wlnigrkda",
   "metadata": {},
   "outputs": [],
   "source": [
    "**Note on AWS Batch Complexity**\n",
    "\n",
    "AWS Batch setup is complex and requires careful configuration of networking, IAM, and compute resources. For easier HPC setups, consider using AWS ParallelCluster or EKS instead. The function above provides a template structure for those who want to implement full Batch integration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parallel-cluster",
   "metadata": {},
   "source": [
    "## Method 3: AWS ParallelCluster Integration\n",
    "\n",
    "AWS ParallelCluster is designed for HPC workloads and integrates well with Clustrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parallelcluster",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Clustrix for ParallelCluster\n",
    "def configure_for_parallelcluster(cluster_name, master_ip):\n",
    "    \"\"\"Configure Clustrix to use AWS ParallelCluster.\"\"\"\n",
    "    configure(\n",
    "        cluster_type=\"slurm\",\n",
    "        cluster_host=master_ip,\n",
    "        username=\"ec2-user\",\n",
    "        key_file=\"~/.ssh/aws-clustrix-key\",\n",
    "        remote_work_dir=\"/shared/clustrix\",  # Use shared storage\n",
    "        package_manager=\"uv\",\n",
    "        module_loads=[\"python3\"],  # Load required modules\n",
    "        default_cores=4,\n",
    "        default_memory=\"8GB\",\n",
    "        default_time=\"01:00:00\",\n",
    "        default_partition=\"compute\"\n",
    "    )\n",
    "    return f\"Configured Clustrix for ParallelCluster: {cluster_name}\"\n",
    "\n",
    "# Example usage:\n",
    "# result = configure_for_parallelcluster(\"my-cluster\", \"10.0.0.100\")\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ipi0is97ue",
   "metadata": {},
   "source": [
    "### ParallelCluster Configuration Example\n",
    "\n",
    "Here's a sample ParallelCluster configuration file for use with Clustrix:\n",
    "\n",
    "```ini\n",
    "# Save as ~/.parallelcluster/config\n",
    "[aws]\n",
    "aws_region_name = us-east-1\n",
    "\n",
    "[global]\n",
    "cluster_template = clustrix-template\n",
    "update_check = false\n",
    "sanity_check = true\n",
    "\n",
    "[cluster clustrix-template]\n",
    "key_name = your-key-name\n",
    "vpc_settings = vpc-settings\n",
    "compute_instance_type = c5.xlarge\n",
    "master_instance_type = t3.medium\n",
    "initial_queue_size = 0\n",
    "max_queue_size = 10\n",
    "scheduler = slurm\n",
    "placement_group = DYNAMIC\n",
    "placement = compute\n",
    "disable_hyperthreading = false\n",
    "post_install = https://raw.githubusercontent.com/your-repo/clustrix-setup.sh\n",
    "\n",
    "[vpc vpc-settings]\n",
    "vpc_id = vpc-xxxxxxxxx\n",
    "master_subnet_id = subnet-xxxxxxxxx\n",
    "compute_subnet_id = subnet-xxxxxxxxx\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "storage-s3",
   "metadata": {},
   "source": [
    "## Data Management with S3\n",
    "\n",
    "Integrate S3 for data input/output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s3-integration",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cluster(cores=2, memory=\"4GB\")\n",
    "def process_s3_data(bucket_name, input_key, output_key):\n",
    "    \"\"\"Process data from S3 and save results back to S3.\"\"\"\n",
    "    import boto3\n",
    "    import numpy as np\n",
    "    import pickle\n",
    "    import io\n",
    "    \n",
    "    s3 = boto3.client('s3')\n",
    "    \n",
    "    # Download data from S3\n",
    "    response = s3.get_object(Bucket=bucket_name, Key=input_key)\n",
    "    data = pickle.loads(response['Body'].read())\n",
    "    \n",
    "    # Process the data\n",
    "    processed_data = {\n",
    "        'original_shape': data.shape if hasattr(data, 'shape') else len(data),\n",
    "        'mean': np.mean(data) if hasattr(data, '__iter__') else data,\n",
    "        'std': np.std(data) if hasattr(data, '__iter__') else 0,\n",
    "        'processing_timestamp': time.time()\n",
    "    }\n",
    "    \n",
    "    # Upload results to S3\n",
    "    output_buffer = io.BytesIO()\n",
    "    pickle.dump(processed_data, output_buffer)\n",
    "    output_buffer.seek(0)\n",
    "    \n",
    "    s3.put_object(\n",
    "        Bucket=bucket_name,\n",
    "        Key=output_key,\n",
    "        Body=output_buffer.getvalue()\n",
    "    )\n",
    "    \n",
    "    return f\"Processed data saved to s3://{bucket_name}/{output_key}\"\n",
    "\n",
    "# Example S3 utility functions\n",
    "def upload_to_s3(data, bucket_name, key):\n",
    "    \"\"\"Upload data to S3.\"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "    buffer = io.BytesIO()\n",
    "    pickle.dump(data, buffer)\n",
    "    buffer.seek(0)\n",
    "    s3.put_object(Bucket=bucket_name, Key=key, Body=buffer.getvalue())\n",
    "    print(f\"✓ Data uploaded to s3://{bucket_name}/{key}\")\n",
    "\n",
    "def download_from_s3(bucket_name, key):\n",
    "    \"\"\"Download data from S3.\"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "    response = s3.get_object(Bucket=bucket_name, Key=key)\n",
    "    data = pickle.loads(response['Body'].read())\n",
    "    print(f\"✓ Data downloaded from s3://{bucket_name}/{key}\")\n",
    "    return data\n",
    "\n",
    "# Example usage:\n",
    "# sample_data = np.random.rand(1000, 100)\n",
    "# upload_to_s3(sample_data, 'your-bucket', 'input/sample_data.pkl')\n",
    "# result = process_s3_data('your-bucket', 'input/sample_data.pkl', 'output/results.pkl')\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "security",
   "metadata": {},
   "source": [
    "## Security Best Practices\n",
    "\n",
    "### Security Group Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "security-group",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clustrix_security_group(vpc_id, your_ip):\n",
    "    \"\"\"\n",
    "    Create a security group for Clustrix with minimal required access.\n",
    "    \n",
    "    Args:\n",
    "        vpc_id: VPC ID where to create the security group\n",
    "        your_ip: Your public IP address (get from https://checkip.amazonaws.com)\n",
    "    \n",
    "    Returns:\n",
    "        Security group ID\n",
    "    \"\"\"\n",
    "    ec2 = boto3.client('ec2')\n",
    "    \n",
    "    try:\n",
    "        response = ec2.create_security_group(\n",
    "            GroupName='clustrix-sg',\n",
    "            Description='Security group for Clustrix compute nodes',\n",
    "            VpcId=vpc_id\n",
    "        )\n",
    "        \n",
    "        sg_id = response['GroupId']\n",
    "        \n",
    "        # Add SSH access from your IP only\n",
    "        ec2.authorize_security_group_ingress(\n",
    "            GroupId=sg_id,\n",
    "            IpPermissions=[\n",
    "                {\n",
    "                    'IpProtocol': 'tcp',\n",
    "                    'FromPort': 22,\n",
    "                    'ToPort': 22,\n",
    "                    'IpRanges': [{'CidrIp': f'{your_ip}/32', 'Description': 'SSH access'}]\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        print(f\"✓ Created security group: {sg_id}\")\n",
    "        return sg_id\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error creating security group: {e}\")\n",
    "        return None\n",
    "\n",
    "# Helper function to get your public IP\n",
    "def get_my_public_ip():\n",
    "    \"\"\"Get your current public IP address.\"\"\"\n",
    "    import requests\n",
    "    try:\n",
    "        response = requests.get('https://checkip.amazonaws.com')\n",
    "        return response.text.strip()\n",
    "    except:\n",
    "        print(\"Could not determine public IP. Please check manually at https://checkip.amazonaws.com\")\n",
    "        return None\n",
    "\n",
    "# Example usage:\n",
    "# my_ip = get_my_public_ip()\n",
    "# if my_ip:\n",
    "#     print(f\"Your public IP: {my_ip}\")\n",
    "#     # sg_id = create_clustrix_security_group('vpc-xxxxxxxxx', my_ip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "or3qhdz81af",
   "metadata": {},
   "outputs": [],
   "source": [
    "### AWS Security Checklist for Clustrix\n",
    "\n",
    "✓ **Authentication & Access**\n",
    "- Use IAM roles instead of access keys when possible\n",
    "- Restrict security groups to your IP address only\n",
    "- Regularly rotate SSH keys and access credentials\n",
    "\n",
    "✓ **Network Security**\n",
    "- Use private subnets for compute nodes when possible\n",
    "- Enable VPC Flow Logs for network monitoring\n",
    "- Use AWS Systems Manager Session Manager instead of direct SSH when possible\n",
    "\n",
    "✓ **Data Protection**\n",
    "- Use encrypted EBS volumes and S3 buckets\n",
    "- Enable CloudTrail for API logging\n",
    "\n",
    "✓ **Monitoring & Management**\n",
    "- Set up billing alerts to monitor costs\n",
    "- Tag all resources for cost tracking and management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cost-optimization",
   "metadata": {},
   "source": [
    "## Cost Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cost-tips",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example spot instance configuration\n",
    "def configure_spot_instances():\n",
    "    \"\"\"Example of using spot instances for cost savings.\"\"\"\n",
    "    configure(\n",
    "        cluster_type=\"ssh\",\n",
    "        cluster_host=\"your-spot-instance-ip\",\n",
    "        username=\"ec2-user\",\n",
    "        key_file=\"~/.ssh/aws-clustrix-key\",\n",
    "        remote_work_dir=\"/tmp/clustrix\",\n",
    "        # Spot instances can be terminated, so use short timeouts\n",
    "        default_time=\"00:30:00\",\n",
    "        job_poll_interval=60,  # Check more frequently\n",
    "        cleanup_on_success=True  # Clean up quickly\n",
    "    )\n",
    "    print(\"✓ Configured for spot instances with appropriate timeouts.\")\n",
    "    return \"Spot instance configuration completed.\"\n",
    "\n",
    "# Example function to check current spot prices\n",
    "def check_spot_prices(instance_types=['t3.large', 't3.xlarge', 'c5.large'], region='us-east-1'):\n",
    "    \"\"\"Check current spot prices for different instance types.\"\"\"\n",
    "    ec2 = boto3.client('ec2', region_name=region)\n",
    "    \n",
    "    try:\n",
    "        response = ec2.describe_spot_price_history(\n",
    "            InstanceTypes=instance_types,\n",
    "            MaxResults=len(instance_types),\n",
    "            ProductDescriptions=['Linux/UNIX']\n",
    "        )\n",
    "        \n",
    "        for price in response['SpotPriceHistory']:\n",
    "            print(f\"{price['InstanceType']}: ${price['SpotPrice']}/hour in {price['AvailabilityZone']}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error checking spot prices: {e}\")\n",
    "\n",
    "# Uncomment to check current spot prices:\n",
    "# check_spot_prices()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gb89uvgkc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### AWS Cost Optimization for Clustrix\n",
    "\n",
    "#### 1. Instance Selection\n",
    "- **Use Spot Instances** for non-critical workloads (up to 90% savings)\n",
    "- **Choose right-sized instances** (don't over-provision)\n",
    "- **Consider AMD instances** (often cheaper than Intel)\n",
    "\n",
    "#### 2. Storage Optimization\n",
    "- Use **S3 Intelligent Tiering** for data\n",
    "- Delete temporary files and logs regularly\n",
    "- Use **gp3 EBS volumes** instead of gp2\n",
    "\n",
    "#### 3. Network Efficiency\n",
    "- Use same AZ for compute and storage to avoid data transfer costs\n",
    "- Minimize cross-region data transfer\n",
    "\n",
    "#### 4. Smart Scheduling\n",
    "- Use scheduled scaling for predictable workloads\n",
    "- Terminate instances when not in use\n",
    "- Use AWS Lambda for small, short-running tasks\n",
    "\n",
    "#### 5. Monitoring & Control\n",
    "- Set up cost alerts and budgets\n",
    "- Use AWS Cost Explorer to analyze spending\n",
    "- Monitor with CloudWatch to optimize resource usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup",
   "metadata": {},
   "source": [
    "## Resource Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup-resources",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_aws_resources(instance_ids=None, security_group_ids=None):\n",
    "    \"\"\"\n",
    "    Clean up AWS resources to avoid ongoing charges.\n",
    "    \n",
    "    Args:\n",
    "        instance_ids: List of EC2 instance IDs to terminate\n",
    "        security_group_ids: List of security group IDs to delete\n",
    "    \"\"\"\n",
    "    ec2 = boto3.client('ec2')\n",
    "    \n",
    "    try:\n",
    "        # Terminate instances\n",
    "        if instance_ids:\n",
    "            response = ec2.terminate_instances(InstanceIds=instance_ids)\n",
    "            print(f\"⏳ Terminating instances: {instance_ids}\")\n",
    "            \n",
    "            # Wait for termination\n",
    "            waiter = ec2.get_waiter('instance_terminated')\n",
    "            waiter.wait(InstanceIds=instance_ids)\n",
    "            print(\"✓ Instances terminated.\")\n",
    "        \n",
    "        # Delete security groups\n",
    "        if security_group_ids:\n",
    "            for sg_id in security_group_ids:\n",
    "                try:\n",
    "                    ec2.delete_security_group(GroupId=sg_id)\n",
    "                    print(f\"✓ Deleted security group: {sg_id}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"✗ Could not delete security group {sg_id}: {e}\")\n",
    "                    \n",
    "        print(\"✅ Cleanup completed!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error during cleanup: {e}\")\n",
    "\n",
    "# Helper function to list your running instances\n",
    "def list_running_instances():\n",
    "    \"\"\"List all running EC2 instances in your account.\"\"\"\n",
    "    ec2 = boto3.client('ec2')\n",
    "    \n",
    "    try:\n",
    "        response = ec2.describe_instances(\n",
    "            Filters=[{'Name': 'instance-state-name', 'Values': ['running']}]\n",
    "        )\n",
    "        \n",
    "        instances = []\n",
    "        for reservation in response['Reservations']:\n",
    "            for instance in reservation['Instances']:\n",
    "                name = next((tag['Value'] for tag in instance.get('Tags', []) if tag['Key'] == 'Name'), 'No Name')\n",
    "                instances.append({\n",
    "                    'InstanceId': instance['InstanceId'],\n",
    "                    'Name': name,\n",
    "                    'InstanceType': instance['InstanceType'],\n",
    "                    'PublicIpAddress': instance.get('PublicIpAddress', 'No Public IP')\n",
    "                })\n",
    "        \n",
    "        if instances:\n",
    "            print(\"Running instances:\")\n",
    "            for inst in instances:\n",
    "                print(f\"  {inst['InstanceId']} ({inst['Name']}) - {inst['InstanceType']} - {inst['PublicIpAddress']}\")\n",
    "        else:\n",
    "            print(\"No running instances found.\")\n",
    "            \n",
    "        return instances\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error listing instances: {e}\")\n",
    "        return []\n",
    "\n",
    "# Example cleanup (uncomment and modify as needed)\n",
    "# instances = list_running_instances()\n",
    "# cleanup_aws_resources(\n",
    "#     instance_ids=['i-1234567890abcdef0'],\n",
    "#     security_group_ids=['sg-1234567890abcdef0']\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5y04rycyarp",
   "metadata": {},
   "outputs": [],
   "source": [
    "**⚠️ Important: Clean Up Resources**\n",
    "\n",
    "Always remember to clean up AWS resources when you're done to avoid ongoing charges! The cleanup function above helps automate this process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-example",
   "metadata": {},
   "source": [
    "## Advanced Example: Distributed Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ml-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cluster(cores=4, memory=\"8GB\", time=\"00:30:00\")\n",
    "def distributed_model_training(data_params, model_params):\n",
    "    \"\"\"\n",
    "    Train a machine learning model on AWS with data from S3.\n",
    "    \n",
    "    Args:\n",
    "        data_params: Dictionary with S3 bucket and key information\n",
    "        model_params: Dictionary with model hyperparameters\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with training results and model location\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import boto3\n",
    "    import pickle\n",
    "    import io\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # Download training data from S3\n",
    "    s3 = boto3.client('s3')\n",
    "    response = s3.get_object(\n",
    "        Bucket=data_params['bucket'], \n",
    "        Key=data_params['training_data_key']\n",
    "    )\n",
    "    data = pickle.loads(response['Body'].read())\n",
    "    \n",
    "    X, y = data['features'], data['labels']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    model = RandomForestClassifier(**model_params)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Save model to S3\n",
    "    model_buffer = io.BytesIO()\n",
    "    pickle.dump(model, model_buffer)\n",
    "    model_buffer.seek(0)\n",
    "    \n",
    "    s3.put_object(\n",
    "        Bucket=data_params['bucket'],\n",
    "        Key=data_params['model_output_key'],\n",
    "        Body=model_buffer.getvalue()\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'model_location': f\"s3://{data_params['bucket']}/{data_params['model_output_key']}\",\n",
    "        'training_samples': len(X_train),\n",
    "        'test_samples': len(X_test)\n",
    "    }\n",
    "\n",
    "# Example usage:\n",
    "# data_config = {\n",
    "#     'bucket': 'your-ml-bucket',\n",
    "#     'training_data_key': 'datasets/training_data.pkl',\n",
    "#     'model_output_key': 'models/random_forest_model.pkl'\n",
    "# }\n",
    "# \n",
    "# model_config = {\n",
    "#     'n_estimators': 100,\n",
    "#     'max_depth': 10,\n",
    "#     'random_state': 42,\n",
    "#     'n_jobs': -1\n",
    "# }\n",
    "# \n",
    "# result = distributed_model_training(data_config, model_config)\n",
    "# print(f\"✓ Model trained with accuracy: {result['accuracy']:.4f}\")\n",
    "# print(f\"✓ Model saved to: {result['model_location']}\")\n",
    "# print(f\"✓ Training samples: {result['training_samples']:,}\")\n",
    "# print(f\"✓ Test samples: {result['test_samples']:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This tutorial covered:\n",
    "\n",
    "1. **Setup**: AWS credentials and Clustrix installation\n",
    "2. **EC2 Integration**: Direct instance configuration\n",
    "3. **AWS Batch**: Managed job scheduling\n",
    "4. **ParallelCluster**: HPC-optimized clusters\n",
    "5. **S3 Integration**: Data storage and retrieval\n",
    "6. **Security**: Best practices for safe deployment\n",
    "7. **Cost Optimization**: Strategies to minimize expenses\n",
    "8. **Resource Management**: Proper cleanup procedures\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Set up your AWS credentials and test the basic configuration\n",
    "- Start with a simple EC2 instance for initial testing\n",
    "- Consider ParallelCluster for production HPC workloads\n",
    "- Implement proper monitoring and cost controls\n",
    "- Explore AWS Spot instances for cost-effective batch processing\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [AWS ParallelCluster Documentation](https://docs.aws.amazon.com/parallelcluster/)\n",
    "- [AWS Batch User Guide](https://docs.aws.amazon.com/batch/)\n",
    "- [AWS HPC Workshops](https://hpc-workshops.com/)\n",
    "- [Clustrix Documentation](https://clustrix.readthedocs.io/)\n",
    "\n",
    "**Remember**: Always monitor your AWS costs and clean up resources when not in use!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
